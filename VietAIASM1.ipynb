{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "V1DwVkHCNDgT",
        "outputId": "330f3e79-2970-4a3f-fbaf-5d0a76a990fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.9/37.9 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m115.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m853.2/853.2 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.0/337.0 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.7/475.7 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m453.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for html2text (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for spider-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-index llama-index-llms-huggingface llama-index-embeddings-huggingface transformers accelerate bitsandbytes llama-index-readers-web"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQsxqqXgTEW8"
      },
      "source": [
        "### Helpful Imports / Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "E-a6tIP3TEW8"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.response.notebook_utils import display_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cqGVadzjTEW9"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXOJQa7FOGr5"
      },
      "source": [
        "### LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qJqUbUKwojPb"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core import Settings\n",
        "from google.colab import userdata\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "open_ai_key = userdata.get('PHANH_API_KEY')\n",
        "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1, api_key=open_ai_key)\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\", api_key=open_ai_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fEFBPc7MyIdI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('PHANH_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdAD1r1XMvdF"
      },
      "source": [
        "## Data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5tnM2-wMvdK",
        "outputId": "40d2fa1c-3435-415c-cdb9-0ec5eb03f923"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwvye1CRHMX-"
      },
      "source": [
        "#### Extract URLs from base url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiU3zihuMvdK"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# Set up headers to mimic a browser request\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "# Function to extract blog data\n",
        "def extract_blog_data(base_url):\n",
        "    try:\n",
        "        r = requests.get(base_url, headers=headers, timeout=10)\n",
        "        r.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "        soup = BeautifulSoup(r.content, 'html.parser')\n",
        "\n",
        "        blog_posts = soup.find_all('div', class_='CardBlog_card__mm0Zw')\n",
        "        blogs = []\n",
        "\n",
        "        for post in blog_posts:\n",
        "            title_elem = post.find('p', class_='CardBlog_title__qC51U')\n",
        "            date_elem = post.find('p', class_='Text_text__zPO0D Text_text-size-16__PkjFu')\n",
        "\n",
        "            if title_elem and date_elem:\n",
        "                title = title_elem.text.strip()\n",
        "                date = date_elem.text.strip()\n",
        "                url = title_elem.find('a')['href'] if title_elem.find('a') else 'No link'\n",
        "                full_url = f\"https://www.llamaindex.ai{url}\"\n",
        "\n",
        "                blogs.append({\n",
        "                    'title': title,\n",
        "                    'date': date,\n",
        "                    'url': full_url\n",
        "                })\n",
        "\n",
        "        return blogs\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return []\n",
        "\n",
        "base_url = 'https://www.llamaindex.ai/blog'\n",
        "blog_data = extract_blog_data(base_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EukJEPA7MvdK",
        "outputId": "5290978d-b57e-48e9-fa6e-1ee635072c72"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'title': 'LlamaIndex Newsletter 2024-07-16',\n",
              "  'date': 'Jul 16, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-16'},\n",
              " {'title': 'Arize AI and LlamaIndex Roll Out Joint Platform for Evaluating LLM Applications',\n",
              "  'date': 'Jul 11, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications'},\n",
              " {'title': 'Case study: Lyzr: Taking autonomous AI agents to $1M+ ARR with LlamaIndex',\n",
              "  'date': 'Jul 10, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex'},\n",
              " {'title': 'LlamaIndex Newsletter 2024-07-09',\n",
              "  'date': 'Jul 9, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-09'},\n",
              " {'title': 'LlamaCloud - Built for Enterprise LLM App Builders',\n",
              "  'date': 'Jul 9, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamacloud-built-for-enterprise-llm-app-builders'},\n",
              " {'title': 'LlamaIndex Newsletter 2024-07-02',\n",
              "  'date': 'Jul 2, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-02'},\n",
              " {'title': 'Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems',\n",
              "  'date': 'Jun 26, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems'},\n",
              " {'title': 'LlamaIndex Newsletter 2024-06-25',\n",
              "  'date': 'Jun 25, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-25'},\n",
              " {'title': 'LlamaIndex Newsletter 2024-06-18',\n",
              "  'date': 'Jun 18, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-18'},\n",
              " {'title': 'Customizing property graph index in LlamaIndex',\n",
              "  'date': 'Jun 11, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/customizing-property-graph-index-in-llamaindex'},\n",
              " {'title': 'LlamaIndex Newsletter 2024-06-11',\n",
              "  'date': 'Jun 11, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-11'},\n",
              " {'title': 'LlamaIndex Newsletter 2024-06-04',\n",
              "  'date': 'Jun 4, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-04'},\n",
              " {'title': 'Introducing the Property Graph Index: A Powerful New Way to Build Knowledge Graphs with LLMs',\n",
              "  'date': 'May 29, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms'},\n",
              " {'title': 'Simplify your RAG application architecture with LlamaIndex + PostgresML',\n",
              "  'date': 'May 28, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/simplify-your-rag-application-architecture-with-llamaindex-postgresml'},\n",
              " {'title': 'LlamaIndex Newsletter 2024-05-28',\n",
              "  'date': 'May 28, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-28'},\n",
              " {'title': 'Automate online tasks with MultiOn and LlamaIndex',\n",
              "  'date': 'May 23, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/automate-online-tasks-with-multion-and-llamaindex'},\n",
              " {'title': 'Batch inference with MyMagic AI and LlamaIndex',\n",
              "  'date': 'May 22, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex'},\n",
              " {'title': 'LlamaIndex Newsletter 2024-05-21',\n",
              "  'date': 'May 21, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-21'},\n",
              " {'title': 'Secure code execution in LlamaIndex with Azure Container Apps dynamic sessions',\n",
              "  'date': 'May 21, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions'},\n",
              " {'title': 'Using LlamaIndex and llamafile to build a local, private research assistant',\n",
              "  'date': 'May 14, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant'},\n",
              " {'title': 'LlamaIndex Newsletter 2024-05-14',\n",
              "  'date': 'May 14, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-14'},\n",
              " {'title': 'LlamaIndex Newsletter 2024-05-07',\n",
              "  'date': 'May 7, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-07'},\n",
              " {'title': 'LlamaIndex Newsletter 2024-04-30',\n",
              "  'date': 'Apr 30, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-30'},\n",
              " {'title': 'Streamlining knowledge work with LlamaIndex, Fireworks and MongoDB',\n",
              "  'date': 'Apr 29, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb'},\n",
              " {'title': 'LlamaIndex Newsletter 2024-04-23',\n",
              "  'date': 'Apr 23, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-23'},\n",
              " {'title': 'LlamaIndex Newsletter 2024-04-16',\n",
              "  'date': 'Apr 16, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-16'},\n",
              " {'title': 'LlamaIndex Newsletter 2024-04-09',\n",
              "  'date': 'Apr 9, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-09'},\n",
              " {'title': 'LlamaIndex Newsletter 2024-04-02',\n",
              "  'date': 'Apr 2, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-02'},\n",
              " {'title': 'LlamaIndex Newsletter 2024-03-26',\n",
              "  'date': 'Mar 26, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-26'},\n",
              " {'title': 'Secure RAG with LlamaIndex and LLM Guard by Protect AI',\n",
              "  'date': 'Mar 20, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/secure-rag-with-llamaindex-and-llm-guard-by-protect-ai'},\n",
              " {'title': 'Retrieving Privacy-Safe Documents Over A Network',\n",
              "  'date': 'Mar 20, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/retrieving-privacy-safe-documents-over-a-network'},\n",
              " {'title': 'Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations',\n",
              "  'date': 'Mar 19, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations'},\n",
              " {'title': 'LlamaIndex Newsletter 2024-03-19',\n",
              "  'date': 'Mar 19, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-19'},\n",
              " {'title': 'One-click Open Source RAG Observability with Langfuse',\n",
              "  'date': 'Mar 18, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/one-click-open-source-rag-observability-with-langfuse'},\n",
              " {'title': 'LlamaIndex Accelerates Enterprise Generative AI with NVIDIA NIM',\n",
              "  'date': 'Mar 18, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-accelerates-enterprise-generative-ai-with-nvidia-nim'},\n",
              " {'title': 'PII Detector: hacking privacy in RAG',\n",
              "  'date': 'Mar 13, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/pii-detector-hacking-privacy-in-rag'},\n",
              " {'title': 'Launching the first GenAI-native document parsing platform',\n",
              "  'date': 'Mar 13, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/launching-the-first-genai-native-document-parsing-platform'},\n",
              " {'title': 'LlamaIndex Newsletter 2024-03-12',\n",
              "  'date': 'Mar 12, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-12'},\n",
              " {'title': 'LlamaIndex Newsletter 2024-03-05',\n",
              "  'date': 'Mar 5, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-05'},\n",
              " {'title': 'Towards Long Context RAG',\n",
              "  'date': 'Mar 1, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/towards-long-context-rag'},\n",
              " {'title': 'Unlocking the 3rd Dimension for Generative AI (Part 1)',\n",
              "  'date': 'Feb 29, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/unlocking-the-3rd-dimension-for-generative-ai-part-1'},\n",
              " {'title': 'Querying a network of knowledge with llama-index-networks',\n",
              "  'date': 'Feb 27, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f'},\n",
              " {'title': 'LlamaIndex Newsletter 2024–02–27',\n",
              "  'date': 'Feb 27, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-27-4b9102a0f824'},\n",
              " {'title': 'Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot',\n",
              "  'date': 'Feb 24, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3'},\n",
              " {'title': 'Introducing LlamaCloud and LlamaParse',\n",
              "  'date': 'Feb 20, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b'},\n",
              " {'title': 'LlamaIndex Newsletter 2024–02–20: introducing LlamaCloud',\n",
              "  'date': 'Feb 20, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4'},\n",
              " {'title': 'MultiModal RAG for Advanced Video Processing with LlamaIndex & LanceDB',\n",
              "  'date': 'Feb 17, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e'},\n",
              " {'title': 'LlamaIndex Newsletter 2023–02–13',\n",
              "  'date': 'Feb 13, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-02-13-26fa79601ba5'},\n",
              " {'title': 'Pioneering the Future of Housing: Introducing GenAI-Driven ADU Planning',\n",
              "  'date': 'Feb 12, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f'},\n",
              " {'title': 'LlamaIndex v0.10',\n",
              "  'date': 'Feb 12, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-v0-10-838e735948f8'},\n",
              " {'title': 'How to build LLM Agents in TypeScript with LlamaIndex.TS',\n",
              "  'date': 'Feb 8, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa'},\n",
              " {'title': 'LlamaIndex Newsletter 2024–02–06',\n",
              "  'date': 'Feb 6, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-02-06-9a303130ad9f'},\n",
              " {'title': 'RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex',\n",
              "  'date': 'Feb 2, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089'},\n",
              " {'title': 'LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG',\n",
              "  'date': 'Jan 31, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00'},\n",
              " {'title': 'Building a Fully Open Source Retriever with Nomic Embed and LlamaIndex',\n",
              "  'date': 'Jan 30, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4'},\n",
              " {'title': 'Agentic RAG With LlamaIndex',\n",
              "  'date': 'Jan 30, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/agentic-rag-with-llamaindex-2721b8a49ff6'},\n",
              " {'title': 'LlamaIndex Newsletter 2024–01–30',\n",
              "  'date': 'Jan 30, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-30-0d01eb0d8cef'},\n",
              " {'title': 'Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex',\n",
              "  'date': 'Jan 26, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9'},\n",
              " {'title': 'Introducing the LlamaIndex retrieval-augmented generation command-line tool',\n",
              "  'date': 'Jan 26, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/introducing-the-llamaindex-retrieval-augmented-generation-command-line-tool-a973fa519a41'},\n",
              " {'title': 'Building Scalable RAG Applications with LlamaIndex and Zilliz Cloud Pipelines',\n",
              "  'date': 'Jan 25, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf'},\n",
              " {'title': 'Building a Slack bot that learns with LlamaIndex, Qdrant and Render',\n",
              "  'date': 'Jan 25, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840'},\n",
              " {'title': 'LlamaIndex Newsletter 2024–01–23',\n",
              "  'date': 'Jan 23, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-23-11ee2c211bab'},\n",
              " {'title': 'LlamaIndex Newsletter 2024–01–16',\n",
              "  'date': 'Jan 16, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-16-752195bed96d'},\n",
              " {'title': 'Building Multi-Tenancy RAG System with LlamaIndex',\n",
              "  'date': 'Jan 15, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b'},\n",
              " {'title': 'AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)',\n",
              "  'date': 'Jan 14, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a'},\n",
              " {'title': 'Free Advanced RAG Certification course with Activeloop and LlamaIndex',\n",
              "  'date': 'Jan 11, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb'},\n",
              " {'title': 'LlamaIndex Newsletter 2024–01–09',\n",
              "  'date': 'Jan 9, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-09-6209000da2e6'},\n",
              " {'title': 'Introducing Query Pipelines',\n",
              "  'date': 'Jan 8, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/introducing-query-pipelines-025dc2bb0537'},\n",
              " {'title': 'A Cheat Sheet and Some Recipes For Building Advanced RAG',\n",
              "  'date': 'Jan 5, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b'},\n",
              " {'title': 'Building An Intelligent Query-Response System with LlamaIndex and OpenLLM',\n",
              "  'date': 'Jan 3, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf'},\n",
              " {'title': 'Scaling LlamaIndex with AWS and Hugging Face',\n",
              "  'date': 'Jan 2, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716'},\n",
              " {'title': 'LlamaIndex Newsletter 2024–01–02',\n",
              "  'date': 'Jan 2, 2024',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-01-02-f349db8c1842'},\n",
              " {'title': 'Running Mixtral 8x7 locally with LlamaIndex and Ollama',\n",
              "  'date': 'Dec 21, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab'},\n",
              " {'title': 'Two new llama-datasets and a Gemini vs. GPT showdown',\n",
              "  'date': 'Dec 20, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5'},\n",
              " {'title': 'LlamaIndex Newsletter 2023–12–19',\n",
              "  'date': 'Dec 19, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-19-2965a2d03726'},\n",
              " {'title': 'Multimodal RAG pipeline with LlamaIndex and Neo4j',\n",
              "  'date': 'Dec 18, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206'},\n",
              " {'title': 'Transforming Natural Language to SQL and Insights for E-commerce with LlamaIndex, GPT3.5, and Streamlit',\n",
              "  'date': 'Dec 17, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9'},\n",
              " {'title': 'LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model',\n",
              "  'date': 'Dec 15, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277'},\n",
              " {'title': 'How to train a custom GPT on your data with EmbedAI + LlamaIndex',\n",
              "  'date': 'Dec 14, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070'},\n",
              " {'title': 'LlamaIndex + Gemini',\n",
              "  'date': 'Dec 13, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e'},\n",
              " {'title': 'LlamaIndex Newsletter 2023–12–12',\n",
              "  'date': 'Dec 12, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-12-4a5d542fbb1e'},\n",
              " {'title': 'Bridging the Language Gap in Programming: Introducing AutoTranslateDoc',\n",
              "  'date': 'Dec 8, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8'},\n",
              " {'title': 'LlamaIndex + Waii: Combining Structured Data from your Database with PDFs for Enhanced Data Analysis',\n",
              "  'date': 'Dec 6, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82'},\n",
              " {'title': 'LlamaIndex Newsletter 2023–12–05',\n",
              "  'date': 'Dec 5, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-05-faf5ab930264'},\n",
              " {'title': 'Introducing Llama Datasets 🦙📝',\n",
              "  'date': 'Dec 4, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/introducing-llama-datasets-aadb9994ad9e'},\n",
              " {'title': 'OpenAI Cookbook: Evaluating RAG systems',\n",
              "  'date': 'Nov 28, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/openai-cookbook-evaluating-rag-systems-fe393c61fb93'},\n",
              " {'title': 'LlamaIndex Newsletter 2023–11–28',\n",
              "  'date': 'Nov 28, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-28-a31be430a786'},\n",
              " {'title': 'Multimodal RAG: Building ‘AInimal Go!’, a Pokémon Go-Inspired App with ResNet, Cohere and Llamaindex',\n",
              "  'date': 'Nov 27, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/multimodal-rag-building-ainimal-go-fecf8404ed97'},\n",
              " {'title': 'Introducing Llama Packs',\n",
              "  'date': 'Nov 22, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/introducing-llama-packs-e14f453b913a'},\n",
              " {'title': 'Introducing RAGs: Your Personalized ChatGPT Experience Over Your Data',\n",
              "  'date': 'Nov 21, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1'},\n",
              " {'title': 'LlamaIndex Newsletter 2023–11–21',\n",
              "  'date': 'Nov 21, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-21-aa3a71e339f8'},\n",
              " {'title': 'Becoming Proficient in Document Extraction',\n",
              "  'date': 'Nov 20, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/becoming-proficient-in-document-extraction-32aa13046ed5'},\n",
              " {'title': 'Shipping your Retrieval-Augmented Generation app to production with create-llama',\n",
              "  'date': 'Nov 20, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d'},\n",
              " {'title': 'GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.',\n",
              "  'date': 'Nov 17, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9'},\n",
              " {'title': 'Evaluating Multi-Modal Retrieval-Augmented Generation',\n",
              "  'date': 'Nov 16, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428'},\n",
              " {'title': 'Improving Retrieval Performance by Fine-tuning Cohere Reranker with LlamaIndex',\n",
              "  'date': 'Nov 16, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b'},\n",
              " {'title': 'Announcing LlamaIndex 0.9',\n",
              "  'date': 'Nov 15, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/announcing-llamaindex-0-9-719f03282945'},\n",
              " {'title': 'create-llama, a command line tool to generate LlamaIndex apps',\n",
              "  'date': 'Nov 14, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191'},\n",
              " {'title': 'LlamaIndex Newsletter 2023–11–14',\n",
              "  'date': 'Nov 14, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-14-dad06ae4284a'},\n",
              " {'title': 'LlamaIndex turns 1!',\n",
              "  'date': 'Nov 13, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-turns-1-f69dcdd45fe3'},\n",
              " {'title': 'Multi-Modal RAG',\n",
              "  'date': 'Nov 10, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea'},\n",
              " {'title': 'Building My Own ChatGPT Vision with PaLM, KOSMOS-2 and LlamaIndex',\n",
              "  'date': 'Nov 8, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566'},\n",
              " {'title': 'LlamaIndex Newsletter 2023-11–07',\n",
              "  'date': 'Nov 8, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-11-07-cf20b9a833aa'},\n",
              " {'title': 'LlamaIndex news special edition: OpenAI developer day!',\n",
              "  'date': 'Nov 7, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-news-special-edition-openai-developer-day-e955f16db4e2'},\n",
              " {'title': 'LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression',\n",
              "  'date': 'Nov 6, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7'},\n",
              " {'title': 'Boosting RAG: Picking the Best Embedding & Reranker models',\n",
              "  'date': 'Nov 3, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83'},\n",
              " {'title': 'LlamaIndex Newsletter 2023–10–31',\n",
              "  'date': 'Oct 31, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-31-36244e2b3f0c'},\n",
              " {'title': 'NewsGPT(Neotice): Summarize news articles with LlamaIndex — Hackathon winning app',\n",
              "  'date': 'Oct 27, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11'},\n",
              " {'title': 'LlamaIndex newsletter 2023–10–24',\n",
              "  'date': 'Oct 24, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-24-4a76204eeaa3'},\n",
              " {'title': 'NVIDIA Research: RAG with Long Context LLMs',\n",
              "  'date': 'Oct 22, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4'},\n",
              " {'title': 'Mastering PDFs: Extracting Sections, Headings, Paragraphs, and Tables with Cutting-Edge Parser',\n",
              "  'date': 'Oct 18, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125'},\n",
              " {'title': 'Improving RAG effectiveness with Retrieval-Augmented Dual Instruction Tuning (RA-DIT)',\n",
              "  'date': 'Oct 18, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d'},\n",
              " {'title': 'How I built the Streamlit LLM Hackathon winning app — FinSight using LlamaIndex.',\n",
              "  'date': 'Oct 17, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0'},\n",
              " {'title': 'LlamaIndex Newsletter 2023–10–17',\n",
              "  'date': 'Oct 17, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-17-33514cbc04a2'},\n",
              " {'title': 'LlamaIndex update 2023–10–10',\n",
              "  'date': 'Oct 10, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-update-2023-10-10-3718a3d19fb9'},\n",
              " {'title': 'Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex',\n",
              "  'date': 'Oct 5, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5'},\n",
              " {'title': 'LlamaIndex + Laurie Voss: an alpaca joins the llamas',\n",
              "  'date': 'Oct 2, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff'},\n",
              " {'title': 'Timescale Vector x LlamaIndex: Making PostgreSQL a Better Vector Database for AI Applications',\n",
              "  'date': 'Sep 27, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0'},\n",
              " {'title': 'LlamaIndex Update — 20/09/2023',\n",
              "  'date': 'Sep 21, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-update-20-09-2023-86ed66f78bac'},\n",
              " {'title': 'LlamaIndex + Vectara',\n",
              "  'date': 'Sep 12, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-vectara-7a3889cd34cb'},\n",
              " {'title': 'LlamaIndex Update — 09/03/2023',\n",
              "  'date': 'Sep 6, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-update-09-03-2023-4a7c21c0f60b'},\n",
              " {'title': 'Fine-Tuning a Linear Adapter for Any Embedding Model',\n",
              "  'date': 'Sep 6, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383'},\n",
              " {'title': 'ChatGPT’s Knowledge is Two Years Old: What to do if you’re building applications?',\n",
              "  'date': 'Sep 1, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c'},\n",
              " {'title': 'Introducing Airbyte sources within LlamaIndex',\n",
              "  'date': 'Aug 29, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/introducing-airbyte-sources-within-llamaindex-42209071722f'},\n",
              " {'title': 'LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases',\n",
              "  'date': 'Aug 29, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af'},\n",
              " {'title': 'Fine-Tuning Embeddings for RAG with Synthetic Data',\n",
              "  'date': 'Aug 25, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971'},\n",
              " {'title': 'LlamaIndex + Metaphor: Towards Automating Knowledge Work with LLMs',\n",
              "  'date': 'Aug 21, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f'},\n",
              " {'title': 'Easily Finetune Llama 2 for Your Text-to-SQL Applications',\n",
              "  'date': 'Aug 17, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d'},\n",
              " {'title': 'LlamaIndex: Harnessing the Power of Text2SQL and RAG to Analyze Product Reviews',\n",
              "  'date': 'Aug 12, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b'},\n",
              " {'title': 'Zep and LlamaIndex: A Vector Store Walkthrough',\n",
              "  'date': 'Aug 11, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc'},\n",
              " {'title': 'LlamaIndex Update — 08/01/2023',\n",
              "  'date': 'Aug 1, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-update-08-01-2023-185514d9b897'},\n",
              " {'title': 'Data Agents + Zapier NLA',\n",
              "  'date': 'Jul 25, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/data-agents-zapier-nla-67146395ce1'},\n",
              " {'title': 'Introducing LlamaIndex.TS',\n",
              "  'date': 'Jul 24, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/introducing-llamaindex-ts-89f41a1f24ab'},\n",
              " {'title': 'Building Better Tools for LLM Agents',\n",
              "  'date': 'Jul 17, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/building-better-tools-for-llm-agents-f8c5a6714f11'},\n",
              " {'title': 'Data Agents',\n",
              "  'date': 'Jul 12, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/data-agents-eed797d7972f'},\n",
              " {'title': 'LlamaIndex Update — 07/11/2023',\n",
              "  'date': 'Jul 10, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-update-07-10-2023-4ceebdab96cb'},\n",
              " {'title': 'LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development',\n",
              "  'date': 'Jul 4, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024'},\n",
              " {'title': 'Special Feature: Berkeley Hackathon Projects (LlamaIndex Prize Winners)',\n",
              "  'date': 'Jun 30, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0'},\n",
              " {'title': 'Enriching LlamaIndex Models with GraphQL and Graph Databases',\n",
              "  'date': 'Jun 30, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7'},\n",
              " {'title': 'Build and Scale a Powerful Query Engine with LlamaIndex and Ray',\n",
              "  'date': 'Jun 27, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4'},\n",
              " {'title': 'LlamaIndex Update — 06/26/2023',\n",
              "  'date': 'Jun 26, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-update-6-26-2023-ed30a9d45f84'},\n",
              " {'title': 'Build and Evaluate LLM Apps with LlamaIndex and TruLens',\n",
              "  'date': 'Jun 23, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c'},\n",
              " {'title': 'Llama Index & Prem AI Join Forces',\n",
              "  'date': 'Jun 23, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llama-index-prem-ai-join-forces-51702fecedec'},\n",
              " {'title': 'LlamaIndex and Weaviate',\n",
              "  'date': 'Jun 22, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-and-weaviate-ba3ff1cbf5f4'},\n",
              " {'title': 'LlamaIndex and Transformers Agents',\n",
              "  'date': 'Jun 8, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-and-transformers-agents-67042ee1d8d6'},\n",
              " {'title': 'Building the data framework for LLMs',\n",
              "  'date': 'Jun 6, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/building-the-data-framework-for-llms-bca068e89e0e'},\n",
              " {'title': 'Vellum <> LlamaIndex Integration',\n",
              "  'date': 'Jun 5, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f'},\n",
              " {'title': 'Combining Text-to-SQL with Semantic Search for Retrieval Augmented Generation',\n",
              "  'date': 'May 28, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b'},\n",
              " {'title': 'Dumber LLM Agents Need More Constraints and Better Tools',\n",
              "  'date': 'May 23, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12'},\n",
              " {'title': 'Build a ChatGPT with your Private Data using LlamaIndex and MongoDB',\n",
              "  'date': 'May 18, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c'},\n",
              " {'title': 'Using LLM’s for Retrieval and Reranking',\n",
              "  'date': 'May 17, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6'},\n",
              " {'title': 'Testing Anthropic Claude’s 100k-token window on SEC 10-K Filings',\n",
              "  'date': 'May 12, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba'},\n",
              " {'title': 'LlamaIndex on TWIML AI: A Distilled Summary (using LlamaIndex)',\n",
              "  'date': 'May 10, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595'},\n",
              " {'title': 'A New Document Summary Index for LLM-powered QA Systems',\n",
              "  'date': 'May 8, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec'},\n",
              " {'title': 'Building and Evaluating a QA System with LlamaIndex',\n",
              "  'date': 'May 7, 2023',\n",
              "  'url': 'https://www.llamaindex.ai/blog/building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1'}]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "blog_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1seWw6rMvdK"
      },
      "source": [
        "#### Save data into file for reuse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkg2kyD8MvdK"
      },
      "outputs": [],
      "source": [
        "# Save blog data to a JSON file\n",
        "import json\n",
        "if blog_data:\n",
        "    with open('/content/drive/MyDrive/VietAI/blog_data.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(blog_data, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Blog data saved to blog_data.json\")\n",
        "else:\n",
        "    print(\"No blog data to save.\")\n",
        "\n",
        "# Optionally, print the number of blog posts extracted\n",
        "print(f\"Number of blog posts extracted: {len(blog_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1Gulo_PMvdL"
      },
      "source": [
        "#### Load data from file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUBZVIS5MvdL",
        "outputId": "db2cc981-2742-4830-cea9-3b155ebcb75f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully loaded 155 blog posts from /content/drive/MyDrive/VietAI/blog_data.json\n",
            "\n",
            "First few blog entries:\n",
            "Title: LlamaIndex Newsletter 2024-07-16\n",
            "Date: Jul 16, 2024\n",
            "URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-16\n",
            "\n",
            "Title: Arize AI and LlamaIndex Roll Out Joint Platform for Evaluating LLM Applications\n",
            "Date: Jul 11, 2024\n",
            "URL: https://www.llamaindex.ai/blog/arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications\n",
            "\n",
            "Title: Case study: Lyzr: Taking autonomous AI agents to $1M+ ARR with LlamaIndex\n",
            "Date: Jul 10, 2024\n",
            "URL: https://www.llamaindex.ai/blog/case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def load_blog_data(file_path='blog_data.json'):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            blog_data = json.load(f)\n",
        "        print(f\"Successfully loaded {len(blog_data)} blog posts from {file_path}\")\n",
        "        return blog_data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File {file_path} not found.\")\n",
        "        return []\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from {file_path}. The file may be corrupted.\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while loading the file: {e}\")\n",
        "        return []\n",
        "\n",
        "# Load the blog data\n",
        "blog_data = load_blog_data('/content/drive/MyDrive/VietAI/blog_data.json')\n",
        "\n",
        "# If you want to see the data, you can print the first few entries\n",
        "if blog_data:\n",
        "    print(\"\\nFirst few blog entries:\")\n",
        "    for entry in blog_data[:3]:  # Print first 3 entries\n",
        "        print(f\"Title: {entry['title']}\")\n",
        "        print(f\"Date: {entry['date']}\")\n",
        "        print(f\"URL: {entry['url']}\")\n",
        "        print()\n",
        "else:\n",
        "    print(\"No blog data was loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyHVrMlSeNoM"
      },
      "source": [
        "# Basic RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzhY_DOSLPUW"
      },
      "source": [
        "## Create Documents from urls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvH4SofsI57c"
      },
      "source": [
        "#### Clean text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Dm1YzX3mOHmO"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def preprocess_text(text):\n",
        "    if not text:  # Check if text is None or empty\n",
        "        return \"No content to process\"\n",
        "    # Normalize newlines\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Add space before capital letters that follow lowercase letters (for camelCase)\n",
        "    #text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
        "\n",
        "    # Remove any remaining HTML tags if any\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "\n",
        "    # Remove non-ASCII characters\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "\n",
        "    #Lower\n",
        "    #text = text.lower()\n",
        "\n",
        "    return text.strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_1t1PA0I9BV"
      },
      "source": [
        "#### Extract documents from each URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyBe9UKMNNvc",
        "outputId": "1a0b51ab-4869-441a-8069-eadd46db4c03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 155 Document objects.\n",
            "\n",
            "First document:\n",
            "Text preview: LlamaIndex Newsletter 2024-07-16  LlamaIndex, Data Framework for LLM ApplicationsEnterpriseOpen sourceCommunityCareersBlog Talk to usEnterpriseOpen sourceCommunityCareersBlog Talk to us 2024 LlamaIndexLlamaIndex  Jul 16, 2024LlamaIndex Newsletter 2024-07-16Hello, Llama Family! Welcome to this weeks edition of the LlamaIndex newsletter! Were thrilled to share some exciting updates about our products, the implementation of GraphRAG, demos that have achieved over $1M in ARR, extensive guides, in-de...\n",
            "Metadata: {'URL': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-16', 'title': 'LlamaIndex Newsletter 2024-07-16', 'date': 'Jul 16, 2024'}\n"
          ]
        }
      ],
      "source": [
        "from llama_index.readers.web import BeautifulSoupWebReader\n",
        "from llama_index.core import Document\n",
        "from bs4 import BeautifulSoup\n",
        "from llama_index.readers.web import SimpleWebPageReader\n",
        "\n",
        "# Now, use BeautifulSoupWebReader to extract documents from each URL\n",
        "reader = BeautifulSoupWebReader()\n",
        "documents = []\n",
        "\n",
        "for blog in blog_data:\n",
        "    url = blog['url']\n",
        "\n",
        "    try:\n",
        "        # Load data from the URL\n",
        "        loaded_document = reader.load_data([url])\n",
        "\n",
        "        if loaded_document:\n",
        "            doc = loaded_document[0] #return list of doc\n",
        "            doc.text = preprocess_text(doc.text)\n",
        "            # Update the metadata with additional information\n",
        "            doc.metadata.update({\n",
        "                'title': blog['title'],\n",
        "                'date': blog['date'],\n",
        "            })\n",
        "\n",
        "            documents.append(doc)\n",
        "        else:\n",
        "            print(f\"No content extracted from {url}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting content from {url}: {e}\")\n",
        "\n",
        "print(f\"Created {len(documents)} Document objects.\")\n",
        "\n",
        "# Optional: Print some information about the first document\n",
        "if documents:\n",
        "    print(\"\\nFirst document:\")\n",
        "    print(f\"Text preview: {documents[0].text[:500]}...\")\n",
        "    print(f\"Metadata: {documents[0].metadata}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "collapsed": true,
        "id": "4-U6fEkJNnMG",
        "outputId": "ff19ef84-c5d8-42ae-a51a-e86d377c0005"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'LlamaIndex Newsletter 2024-07-16  LlamaIndex, Data Framework for LLM ApplicationsEnterpriseOpen sourceCommunityCareersBlog Talk to usEnterpriseOpen sourceCommunityCareersBlog Talk to us 2024 LlamaIndexLlamaIndex  Jul 16, 2024LlamaIndex Newsletter 2024-07-16Hello, Llama Family! Welcome to this weeks edition of the LlamaIndex newsletter! Were thrilled to share some exciting updates about our products, the implementation of GraphRAG, demos that have achieved over $1M in ARR, extensive guides, in-depth tutorials, and hackathons.Before we get into the details of our newsletter, were thrilled to share the beta launch of LlamaCloud. This new data processing layer boosts RAG workflows with sophisticated parsing, indexing, and retrieval functions. Alongside this, were also introducing LlamaTrace in partnership with Arize AI, which provides unmatched tracing, observability, and evaluation capabilities for LLM application workflows.Signup here: cloud.llamaindex.ai The highlights:LlamaCloud Launch: Weve launched the beta release of LlamaCloud, a data processing layer designed to enhance RAG workflows with state-of-the-art parsing, indexing, and retrieval capabilities. Blogpost, Tweet.LlamaTrace Launch: In collaboration with Arize AI, weve introduced LlamaTrace, offering unmatched tracing, observability, and evaluation capabilities for LLM application workflows. It features detailed call stack tracing, one-click setup through LlamaIndex, and seamless integration with LlamaCloud. Blogpost, Tweet.GraphRAG Implementation: Implementation of GraphRAG with LlamaIndex, focusing on graph generation, community building, summaries, and community-based retrieval to improve answer aggregation. Notebook, Tweet.Redis Queue Integration with Llama-Agents: We have integrated Redis Queue with llama-agents to boost coordination and communication in multi-agent workflows, ensuring robust performance. Notebook, Tweet. Feature Releases and Enhancements:We have launched the beta release of LlamaCloud, a data processing layer that enhances RAG workflows with advanced parsing, indexing, and retrieval capabilities. Blogpost, Tweet.We have launched an implementation[beta] of GraphRAG concepts with LlamaIndex focussing on graph generation, building communities and community summaries, and community-based retrieval to aggregate answers from summaries. Notebook, Tweet.We have integrated Redis Queue with llama-agents to enhance coordination in multi-agent workflows, allowing for robust communication. Notebook, Tweet.We have introduced LlamaTrace in collaboration with Arize AI, offering unparalleled tracing, observability, and evaluation capabilities for LLM application workflows. LlamaTrace stands out for its detailed tracing, which logs the entire call stack, one-click setup through LlamaIndex, and seamless integration with LlamaCloud for easy access and authentication. Blogpost, Tweet.We have integrated NebulaGraph with LlamaIndex, enhancing PropertyGraph capabilities with sophisticated extractors, customizable properties on nodes and edges, and advanced retrieval options. Docs, Tweet. Demos:Lyzrai has achieved over $1M ARR using LlamaIndex! This full-stack autonomous AI agent framework enhances AI sales and marketing functions with LlamaIndexs data connectors and RAG capabilities, boasting rapid revenue growth, high accuracy, and customer satisfaction. Guides:Guide to Multi-Modal RAG for Document Processing that introduces a multi-modal RAG architecture using LlamaParse, LlamaIndex, and GPT-4o, designed to handle complex slide decks. Tweet.Guide to using LlamaParse and GPT-4o for Financial Report RAG to to effectively parse and synthesize complex financial documents, enhancing clarity and accuracy in data analysis.Guide to Building Agentic RAG with Llama3: Explore our comprehensive cookbooks, created in collaboration with AI at Meta, featuring advanced techniques from routing and tool use to constructing complex agent reasoning loops and multi-document agents using purely local models like Llama3. Tutorials:1LittleCoders video tutorial demonstrates how to deploy self-hosted llama-agents using Arcee AI, MistralAI, and Ollama, including setup, local model integration, and tool development.kingzzms tutorial on using LlamaIndex to build advanced RAG flows, detailing how to compose and visualize each step from basic retrieval and prompting to advanced techniques and evaluation with RAGAS.Mervin Praisons tutorial on using llama-agents, detailing the frameworks purpose, a step-by-step setup guide for multi-agent services, and how it stands out from other frameworks. Events:Join our online hackathon this Friday, 19th, to build AI apps with Llama 3 from Meta and win cash, credits, and prizes from us and our co-hosts TogetherAI, Milvus, and LablabAI.LlamaIndexBlogPartnersCareersContactStatusEnterpriseLlamaCloudLlamaParseOpen SourcePython packagePython docsTypeScript packageTypeScript docsLlamaHubGitHubCommunityNewsletterDiscordTwitter/XLinkedInYouTubeStarter projectscreate-llamaSEC InsightsChat LlamaIndexLlamaBotRAG CLI 2024 LlamaIndexPrivacy NoticeTerms of Service'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[0].text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwgArO-eLUqF"
      },
      "source": [
        "## Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "sY9GpqQIPBiF"
      },
      "outputs": [],
      "source": [
        "# chunking\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "splitter = SentenceSplitter(\n",
        "    chunk_size=1024,\n",
        "    chunk_overlap=20,\n",
        ")\n",
        "\n",
        "Settings.text_splitter = splitter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpG5sDloPQ8V",
        "outputId": "34958288-6b79-4380-d353-fb4cb5a32ce8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "412"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nodes = splitter.get_nodes_from_documents(documents)\n",
        "len(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DuLUGwi8Sgy5",
        "outputId": "803e50d6-e7f2-46f9-8a50-70f23072b62a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TextNode(id_='ee21d038-6fbe-49d9-b685-bb939e8eccbf', embedding=None, metadata={'URL': 'https://www.llamaindex.ai/blog/arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications', 'title': 'Arize AI and LlamaIndex Roll Out Joint Platform for Evaluating LLM Applications', 'date': 'Jul 11, 2024'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='https://www.llamaindex.ai/blog/arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'URL': 'https://www.llamaindex.ai/blog/arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications', 'title': 'Arize AI and LlamaIndex Roll Out Joint Platform for Evaluating LLM Applications', 'date': 'Jul 11, 2024'}, hash='b4356f1e0fa67eb529549febac2ccace4f6a457b35caff1744912ff6008ecd12')}, text='Arize AI and LlamaIndex Roll Out Joint Platform for Evaluating LLM Applications  LlamaIndex, Data Framework for LLM ApplicationsEnterpriseOpen sourceCommunityCareersBlog Talk to usEnterpriseOpen sourceCommunityCareersBlog Talk to us 2024 LlamaIndexLlamaIndex  Jul 11, 2024Arize AI and LlamaIndex Roll Out Joint Platform for Evaluating LLM ApplicationsEvaluationObservabilityStrategic alliance and joint product promises to broaden the adoption of generative AI across industriesArize AI, a pioneer and leader in AI observability and LLM evaluation, and LlamaIndex, a leading data framework for LLM applications, debuted a new joint offering today called LlamaTrace, a hosted version of Arize OSS Phoenix.According to a soon-to-release survey, 47.7% of AI engineers and developers building generative AI applications are leveraging retrieval today in their LLM Applications. By connecting data to generative AI, orchestration frameworks like LlamaIndex can be game-changers in accelerating generative AI development. However, for many teams and enterprises technical challenges remain in getting modern LLM systems  with layers of abstraction  ready for the real world.To help, Arize and LlamaIndex are debuting an LLM tracing and observability platform that works natively with the LlamaIndex and Arize ecosystem. With a foundation based on Arize Phoenix OSS, the hosted version of Phoenix offers the ability to persist application telemetry data generated during AI development in order to better experiment, iterate, and collaborate in development or production.The solution has a foundation in open source and features a fully hosted, online, persistent deployment option for teams that do not want to self host. AI engineers can instantly log traces, persist datasets, run experiments, run evaluations  and share those insights with colleagues.The new offering is available today, and can be accessed through either a LlamaIndex or Arize account.We share a vision with LlamaIndex in enabling builders to reduce the time it takes to deploy generative AI into production but in a way that is super battle hardened for business-critical use cases, said Jason Lopatecki, CEO and Co-Founder of Arize. As leaders in our respective spaces with a common philosophy in empowering AI engineers and developers, were uniquely positioned here to do something that can move modern LLMOps forward and broaden adoption.Prototyping a RAG pipeline or agent is easy, but every AI engineer needs the right data processing layer, orchestration framework, and experimentation/monitoring tool in order to take these applications to production. LlamaTrace by Arize offers the richest toolkit weve seen in enabling developers to observe, debug, and evaluate every granular step of a very complex LLM workflow, and it nicely complements the production-ready data platform and orchestration framework that LlamaCloud and LlamaIndex offer. - Jerry Liu, CEO of LlamaIndexAbout Arize AIArize AI is an AI observability and LLM evaluation platform that helps teams deliver and maintain more successful AI in production. Arizes automated monitoring and observability platform allows teams to quickly detect issues when they emerge, troubleshoot why they happened, and improve overall performance across both traditional ML and generative use cases. Arize is headquartered in Berkeley, CA.Related articlesArize AI and LlamaIndex Roll Out Joint Platform for Evaluating LLM Applications2024-07-11Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations2024-03-19One-click Open Source RAG Observability with Langfuse2024-03-18Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex2024-01-26LlamaIndexBlogPartnersCareersContactStatusEnterpriseLlamaCloudLlamaParseOpen SourcePython packagePython docsTypeScript packageTypeScript docsLlamaHubGitHubCommunityNewsletterDiscordTwitter/XLinkedInYouTubeStarter projectscreate-llamaSEC InsightsChat LlamaIndexLlamaBotRAG CLI 2024 LlamaIndexPrivacy NoticeTerms of Service', mimetype='text/plain', start_char_idx=0, end_char_idx=4007, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nodes[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YRLGesMANtO"
      },
      "source": [
        "## Query with ChromaDb (Persitant index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7J8mcCLrOAKE",
        "outputId": "c442a724-96e6-444e-f5e9-9ce027250d61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.3/584.3 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-index-vector-stores-chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5fbhlWxjRo0T",
        "outputId": "5e78618d-2e0c-485f-f3fa-567d12bebb94"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Dm8k0xONXBr",
        "outputId": "8392f936-4054-4cfb-db5e-c6d37c098c9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/VietAI\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/VietAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "GVGRM5G4OV4I"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "import chromadb\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import StorageContext\n",
        "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
        "from llama_index.core import SummaryIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "HpM6DUWEP7Mm"
      },
      "outputs": [],
      "source": [
        "# Set up ChromaDB client (adjust the path to your Google Drive)\n",
        "db_path = \"./chroma_DB\"\n",
        "db = chromadb.PersistentClient(path=db_path)\n",
        "\n",
        "db.delete_collection(\"llamaindex_collection_new\")\n",
        "\n",
        "# Get or create collection\n",
        "chroma_collection = db.create_collection(\"llamaindex_collection_new\")\n",
        "# Set up vector store\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "# Create a StorageContext\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "# Set up index\n",
        "index = VectorStoreIndex(nodes, storage_context=storage_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "DaNsBcD8FVwk"
      },
      "outputs": [],
      "source": [
        "# load\n",
        "db_path = \"./chroma_DB\"\n",
        "db2 = chromadb.PersistentClient(path=db_path)\n",
        "chroma_collection = db2.get_or_create_collection(\"llamaindex_collection_new\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "chroma_index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IuZ1vLQO57a"
      },
      "source": [
        "## Get title and position of extracted text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "e1Mzk3fNPLEL"
      },
      "outputs": [],
      "source": [
        "def query_with_sources(query_engine=query_engine, query=\"\"):\n",
        "\n",
        "    # Query the index\n",
        "    response = query_engine.query(query)\n",
        "    print(\"Sources:\")\n",
        "    for node in response.source_nodes:\n",
        "\n",
        "        # Get the title from metadata\n",
        "        title = node.node.metadata.get('title', 'Unknown Title')\n",
        "\n",
        "        # Get the URL from metadata\n",
        "        url = node.node.metadata.get('URL', 'URL not available')\n",
        "\n",
        "        # Get the full text of the node\n",
        "        full_text = node.node.text\n",
        "\n",
        "\n",
        "        print(f\"Title: {title}\")\n",
        "        print(f\" URL: {url}\")\n",
        "        print(f\"Text preview: {full_text[:200]} \\n\\n\")\n",
        "    display_response(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "octmuHQqTZZd"
      },
      "source": [
        "## Answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh3wiG3DerRL"
      },
      "source": [
        "#### Refine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "32_YdLmiKzXo"
      },
      "outputs": [],
      "source": [
        " # Define query engine\n",
        "query_engine = index.as_query_engine(response_mode='refine')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "NY5CgZS9OuAw",
        "outputId": "c6e51046-95c9-42a4-f481-1385881b3e67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sources:\n",
            "Title: Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
            " URL: https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems\n",
            "Text preview: Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems  LlamaIndex, Data Framework for LLM ApplicationsEnterpriseOpen sourceCommunityCareersBlog Talk to usEnterp \n",
            "\n",
            "\n",
            "Title: Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
            " URL: https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems\n",
            "Text preview: His name is Peter.\")] index = VectorStoreIndex.from_documents(docs) # Define a query rewrite agent HYDE_PROMPT_STR = ( \"Please rewrite the following query to include more detail:\\n{query_str}\\n\" ) HYD \n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**`Final Response:`** Distributed Service Oriented Architecture, Communication via standardized API interfaces, Define agentic and explicit orchestration flows, Ease of deployment, Scalability and resource management."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = query_with_sources(query_engine, \"What are key features of llama-agents?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "-XVWEII7P-xb",
        "outputId": "6dfa15aa-521d-4284-a4af-05be1aa08769"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sources:\n",
            "Title: OpenAI Cookbook: Evaluating RAG systems\n",
            " URL: https://www.llamaindex.ai/blog/openai-cookbook-evaluating-rag-systems-fe393c61fb93\n",
            "Text preview: OpenAI Cookbook: Evaluating RAG systems  LlamaIndex, Data Framework for LLM ApplicationsEnterpriseOpen sourceCommunityCareersBlog Talk to usEnterpriseOpen sourceCommunityCareersBlog Talk to us 2024 Ll \n",
            "\n",
            "\n",
            "Title: A Cheat Sheet and Some Recipes For Building Advanced RAG\n",
            " URL: https://www.llamaindex.ai/blog/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b\n",
            "Text preview: A Cheat Sheet and Some Recipes For Building Advanced RAG  LlamaIndex, Data Framework for LLM ApplicationsEnterpriseOpen sourceCommunityCareersBlog Talk to usEnterpriseOpen sourceCommunityCareersBlog T \n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**`Final Response:`** The two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook are the Retrieval System and Response Generation."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = query_with_sources(query_engine,\"What are the two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook??\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "PqCvFNbWPDVp",
        "outputId": "3db6790e-e19c-439c-c431-3abd44b9dd07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sources:\n",
            "Title: Boosting RAG: Picking the Best Embedding & Reranker models\n",
            " URL: https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83\n",
            "Text preview: Implemented by the user. \"\"\" return self._retrieve(query_bundle) async def aretrieve(self, str_or_query_bundle: QueryType) -&gt; List[NodeWithScore]: if isinstance(str_or_query_bundle, str): str_or_qu \n",
            "\n",
            "\n",
            "Title: Boosting RAG: Picking the Best Embedding & Reranker models\n",
            " URL: https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83\n",
            "Text preview: Nearly all embeddings benefit from reranking, showing improved hit rates and MRRs.Rerankers, especially CohereRerank, have demonstrated their capability to transform any embedding into a competitive o \n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**`Final Response:`** The two main metrics used to evaluate the performance of the different rerankers in the RAG system are Mean Reciprocal Rank (MRR) and Hit Rate."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = query_with_sources(query_engine,\"What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_USyKYIewt_"
      },
      "source": [
        "### Tree Summarize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "RLloeu31ezXJ"
      },
      "outputs": [],
      "source": [
        " # Define query engine\n",
        "query_engine = chroma_index.as_query_engine(response_mode='tree_summarize')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "WNIepa3Ee4IV",
        "outputId": "2faef3c6-8f62-4818-bc3f-cc446db86c61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sources:\n",
            "Title: Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
            " URL: https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems\n",
            "Text preview: Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems  LlamaIndex, Data Framework for LLM ApplicationsEnterpriseOpen sourceCommunityCareersBlog Talk to usEnterp \n",
            "\n",
            "\n",
            "Title: Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
            " URL: https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems\n",
            "Text preview: His name is Peter.\")] index = VectorStoreIndex.from_documents(docs) # Define a query rewrite agent HYDE_PROMPT_STR = ( \"Please rewrite the following query to include more detail:\\n{query_str}\\n\" ) HYD \n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**`Final Response:`** The key features of llama-agents include a Distributed Service Oriented Architecture where each agent can function as an independently running microservice, communication via standardized API interfaces facilitated by a central control plane orchestrator, the ability to define agentic and explicit orchestration flows, ease of deployment with the capability to launch, scale, and monitor each agent and control plane independently, and scalability and resource management through built-in observability tools to monitor system quality and performance."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "esponse = query_with_sources(query_engine,\"What are key features of llama-agents?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "KDf8EwosfB8s",
        "outputId": "c54a3aa4-f3af-495e-ebd1-a60e7952bfb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sources:\n",
            "Title: OpenAI Cookbook: Evaluating RAG systems\n",
            " URL: https://www.llamaindex.ai/blog/openai-cookbook-evaluating-rag-systems-fe393c61fb93\n",
            "Text preview: openai cookbook: evaluating rag systems  llamaindex, data framework for llm applicationsenterpriseopen sourcecommunitycareersblog talk to usenterpriseopen sourcecommunitycareersblog talk to us 2024 ll \n",
            "\n",
            "\n",
            "Title: A Cheat Sheet and Some Recipes For Building Advanced RAG\n",
            " URL: https://www.llamaindex.ai/blog/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b\n",
            "Text preview: irrelevant information).llamaindex information compression recipe (notebook guide):from llama_index import simpledirectoryreader, vectorstoreindex from llama_index.query_engine import retrieverqueryen \n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**`Final Response:`** The two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook are the retrieval system and response generation."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = query_with_sources(query_engine,\"What are the two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook??\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "-JMJE3fvfI3s",
        "outputId": "93de3603-3d5f-4bb3-d06f-e3f12af1b950"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sources:\n",
            "Title: Boosting RAG: Picking the Best Embedding & Reranker models\n",
            " URL: https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83\n",
            "Text preview: however, actual outcomes may differ based on data characteristics, dataset size, and other variables like chunk_size, similarity_top_k, and so on.the table below showcases the evaluation results based \n",
            "\n",
            "\n",
            "Title: Boosting RAG: Picking the Best Embedding & Reranker models\n",
            " URL: https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83\n",
            "Text preview: boosting rag: picking the best embedding & reranker models  llamaindex, data framework for llm applicationsenterpriseopen sourcecommunitycareersblog talk to usenterpriseopen sourcecommunitycareersblog \n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**`Final Response:`** The two main metrics used to evaluate the performance of the different rerankers in the RAG system are hit rate and mean reciprocal rank (MRR)."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = query_with_sources(query_engine,\"What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UDmXHlftKXn"
      },
      "source": [
        "## Multiple index with persistant docstore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kon1rAVexLtH"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOUsr7A7tM-D"
      },
      "outputs": [],
      "source": [
        "#putting nodes in the docstore\n",
        "# this allows you to define multiple indices over the same underlying docstore, instead of duplicating data across indices.\n",
        "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
        "\n",
        "doc_store = SimpleDocumentStore()\n",
        "doc_store.add_documents(nodes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8--LdU17vDVJ"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import StorageContext\n",
        "from llama_index.core import VectorStoreIndex, SimpleKeywordTableIndex\n",
        "from llama_index.core import SummaryIndex\n",
        "from llama_index.core import ComposableGraph\n",
        "\n",
        "storage_context = StorageContext.from_defaults(docstore=doc_store)\n",
        "#creating three different indices that all reference the same docstore through the storage context.\n",
        "summary_index = SummaryIndex(nodes, storage_context=storage_context)\n",
        "vector_index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
        "keyword_table_index = SimpleKeywordTableIndex(nodes, storage_context=storage_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3f06eoHDiSl"
      },
      "outputs": [],
      "source": [
        "# Set index id\n",
        "summary_index.set_index_id(\"summary_index\")\n",
        "vector_index.set_index_id(\"vector_index\")\n",
        "keyword_table_index.set_index_id(\"keyword_table_index\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Axpb5N9xAuZv"
      },
      "outputs": [],
      "source": [
        "# Persist the overall storage_context\n",
        "storage_context.persist(persist_dir=\"/content/drive/MyDrive/VietAI/index_store/overall_context\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10wlrUKBxDFI",
        "outputId": "e63d4934-a757-4ac6-a76f-d1f69ed99263"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "226"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# NOTE: the docstore sitll has the same nodes\n",
        "len(storage_context.docstore.docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAkMeAHFAttU"
      },
      "outputs": [],
      "source": [
        "# try to load index from storage\n",
        "from llama_index.core import (\n",
        "    load_index_from_storage,\n",
        "    load_indices_from_storage,\n",
        "    load_graph_from_storage,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "P3v6th1WU9VC",
        "outputId": "19106ac4-53fe-4d51-ec9d-6d5e25155ed0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sources:\n",
            "Title: Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
            " URL: https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems\n",
            "Text preview: his name is peter.\")] index = vectorstoreindex.from_documents(docs) # define a query rewrite agent hyde_prompt_str = ( \"please rewrite the following query to include more detail:\\n{query_str}\\n\" ) hyd \n",
            "\n",
            "\n",
            "Title: Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
            " URL: https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems\n",
            "Text preview: introducing llama-agents: a powerful framework for building production multi-agent ai systems  llamaindex, data framework for llm applicationsenterpriseopen sourcecommunitycareersblog talk to usenterp \n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**`Final Response:`** The key features of llama-agents include a distributed service-oriented architecture where each agent can function as an independent microservice, communication via standardized API interfaces facilitated by a central control plane orchestrator, the ability to define agentic and explicit orchestration flows, ease of deployment allowing for launching, scaling, and monitoring each agent and the control plane independently, as well as built-in observability tools for monitoring the quality and performance of the system and individual agent services."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# try vector_index\n",
        "index = load_index_from_storage(storage_context, index_id=\"vector_index\")\n",
        "\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_with_sources(\"What are key features of llama-agents?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0wX0IwDQvhu"
      },
      "source": [
        "### Load multiple indices from storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "qo9-7CphMiyD"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import StorageContext\n",
        "# Path where you saved your index\n",
        "persist_dir = \"/content/drive/MyDrive/VietAI/index_store/overall_context\"\n",
        "\n",
        "# Load the storage context\n",
        "storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
        "\n",
        "from llama_index.core import get_response_synthesizer\n",
        "# try to load load indexes from storage\n",
        "from llama_index.core import (\n",
        "    load_index_from_storage,\n",
        "    load_indices_from_storage,\n",
        "    load_graph_from_storage,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iYfxAHjDQ1C0",
        "outputId": "028f19ca-9ef8-4870-acf2-61789ed170ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sources:\n",
            "Title: Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
            " URL: https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems\n",
            "Text preview: his name is peter.\")] index = vectorstoreindex.from_documents(docs) # define a query rewrite agent hyde_prompt_str = ( \"please rewrite the following query to include more detail:\\n{query_str}\\n\" ) hyd \n",
            "\n",
            "\n",
            "Title: Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
            " URL: https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems\n",
            "Text preview: introducing llama-agents: a powerful framework for building production multi-agent ai systems  llamaindex, data framework for llm applicationsenterpriseopen sourcecommunitycareersblog talk to usenterp \n",
            "\n",
            "\n",
            "Title: LlamaIndex Newsletter 2024-07-02\n",
            " URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-02\n",
            "Text preview: llamaindex newsletter 2024-07-02  llamaindex, data framework for llm applicationsenterpriseopen sourcecommunitycareersblog talk to usenterpriseopen sourcecommunitycareersblog talk to us 2024 llamainde \n",
            "\n",
            "\n",
            "Title: LlamaIndex Update — 08/01/2023\n",
            " URL: https://www.llamaindex.ai/blog/llamaindex-update-08-01-2023-185514d9b897\n",
            "Text preview: llamaindex update  08/01/2023  llamaindex, data framework for llm applicationsenterpriseopen sourcecommunitycareersblog talk to usenterpriseopen sourcecommunitycareersblog talk to us 2024 llamaindexra \n",
            "\n",
            "\n",
            "Title: LlamaIndex and Transformers Agents\n",
            " URL: https://www.llamaindex.ai/blog/llamaindex-and-transformers-agents-67042ee1d8d6\n",
            "Text preview: using a vector database created from diffusiondb, llamaindex can suggest better prompts when generating images.custom tools in transformers agents are easily distributed and shared using hugging face  \n",
            "\n",
            "\n",
            "Title: Data Agents + Zapier NLA\n",
            " URL: https://www.llamaindex.ai/blog/data-agents-zapier-nla-67146395ce1\n",
            "Text preview: data agents + zapier nla  llamaindex, data framework for llm applicationsenterpriseopen sourcecommunitycareersblog talk to usenterpriseopen sourcecommunitycareersblog talk to us 2024 llamaindexsimon s \n",
            "\n",
            "\n",
            "Title: Dumber LLM Agents Need More Constraints and Better Tools\n",
            " URL: https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12\n",
            "Text preview: dumber llm agents need more constraints and better tools  llamaindex, data framework for llm applicationsenterpriseopen sourcecommunitycareersblog talk to usenterpriseopen sourcecommunitycareersblog t \n",
            "\n",
            "\n",
            "Title: Building Better Tools for LLM Agents\n",
            " URL: https://www.llamaindex.ai/blog/building-better-tools-for-llm-agents-f8c5a6714f11\n",
            "Text preview: building better tools for llm agents  llamaindex, data framework for llm applicationsenterpriseopen sourcecommunitycareersblog talk to usenterpriseopen sourcecommunitycareersblog talk to us 2024 llama \n",
            "\n",
            "\n",
            "Title: Data Agents\n",
            " URL: https://www.llamaindex.ai/blog/data-agents-eed797d7972f\n",
            "Text preview: for instance, you may want to define a specific workflow over text-to-sql with our nlstructstorequeryengine (constrained), or a router module to decide between semantic search or summarization (less c \n",
            "\n",
            "\n",
            "Title: Case study: Lyzr: Taking autonomous AI agents to $1M+ ARR with LlamaIndex\n",
            " URL: https://www.llamaindex.ai/blog/case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex\n",
            "Text preview: case study: lyzr: taking autonomous ai agents to $1m+ arr with llamaindex  llamaindex, data framework for llm applicationsenterpriseopen sourcecommunitycareersblog talk to usenterpriseopen sourcecommu \n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**`Final Response:`** The key features of llama-agents include:\n",
              "1. Distributed service-oriented architecture.\n",
              "2. Communication via standardized API interfaces.\n",
              "3. Defining agentic and explicit orchestration flows.\n",
              "4. Ease of deployment for launching, scaling, and monitoring each agent and the control plane independently.\n",
              "5. Scalability and resource management with built-in observability tools to monitor system performance."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "index = load_index_from_storage(storage_context, index_id=\"keyword_table_index\")\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_with_sources(query_engine, \"What are key features of llama-agents?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73Pw7seGLcQo"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZHeeQ7iLqOO"
      },
      "outputs": [],
      "source": [
        "# enter your question here\n",
        "question1 = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e6BWlymNnLu"
      },
      "outputs": [],
      "source": [
        "question2 = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jA2VCKGmLgKB"
      },
      "outputs": [],
      "source": [
        "index = load_index_from_storage(storage_context, index_id=\"vector_index\")\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_with_sources(query_engine, question1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RC9XrrlRNpbB"
      },
      "outputs": [],
      "source": [
        "response = query_with_sources(query_engine, question2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gus_UCzPU3fY"
      },
      "source": [
        "# Avanced Rag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6pdzpGRJrxd"
      },
      "source": [
        "## Different chunking techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoqPTkI3HqAX"
      },
      "source": [
        "#### TokenTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HT7mFNFtJyeR",
        "outputId": "a3de16c3-7a10-45e0-dce8-d4328cdec245"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "401"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from llama_index.core.node_parser import TokenTextSplitter\n",
        "\n",
        "splitter = TokenTextSplitter(\n",
        "    chunk_size=1024,\n",
        "    chunk_overlap=20,\n",
        "    separator=\" \",\n",
        ")\n",
        "nodes = splitter.get_nodes_from_documents(documents)\n",
        "len(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "cRLiwkBG0R0C",
        "outputId": "0ba54c4b-9aaf-48cc-a9d2-6e10951679d5"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**`Final Response:`** The key features of llama-agents include a Distributed Service Oriented Architecture where each agent can function as an independently running microservice, communication via standardized API interfaces facilitated by a central control plane orchestrator, the ability to define agentic and explicit orchestration flows, ease of deployment with the capability to launch, scale, and monitor each agent and control plane independently, as well as scalability and resource management through built-in observability tools to monitor system performance and quality."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "vector_index = VectorStoreIndex(nodes)\n",
        "query_engine = vector_index.as_query_engine(response_mode=\"refine\")\n",
        "\n",
        "response = query_engine.query(\"What are key features of llama-agents?\")\n",
        "\n",
        "display_response(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLoskNEpKKwm"
      },
      "source": [
        "#### HierarchicalNodeParser\n",
        " It chunks input data into several layers of hierarchy, with each node linked to its parent. This structure is particularly useful when combined with the AutoMergingRetriever, as it facilitates the automatic replacement of retrieved nodes with their parent nodes when a majority of child nodes are retrieved. This process ensures that the LLM has access to a more complete context for response synthesis, enhancing the accuracy and relevance of the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1OWlBPwKIVS",
        "outputId": "f8b96ec1-53e1-459d-911c-a1d4d07dfe5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "914"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from llama_index.core.node_parser import HierarchicalNodeParser\n",
        "\n",
        "node_parser = HierarchicalNodeParser.from_defaults(\n",
        "    chunk_sizes=[3048, 2048, 1000]\n",
        ")\n",
        "\n",
        "nodes = node_parser.get_nodes_from_documents(documents)\n",
        "len(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fY5WrMm2Tj15",
        "outputId": "0ec922ba-2356-42b9-fd8f-9811e9d006aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TextNode(id_='34dea513-3d1b-45d3-a1b1-8f2706522e33', embedding=None, metadata={'URL': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-16', 'title': 'LlamaIndex Newsletter 2024-07-16', 'date': 'Jul 16, 2024'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-16', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'URL': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-16', 'title': 'LlamaIndex Newsletter 2024-07-16', 'date': 'Jul 16, 2024'}, hash='9cd4f2088c78da703ada5910cfb265b0d37b6f8a35ff7318a16eb083db666369'), <NodeRelationship.CHILD: '5'>: [RelatedNodeInfo(node_id='9bf86a64-b01a-4dee-bb09-33e157efdcb4', node_type=<ObjectType.TEXT: '1'>, metadata={'URL': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-16', 'title': 'LlamaIndex Newsletter 2024-07-16', 'date': 'Jul 16, 2024'}, hash='9cd4f2088c78da703ada5910cfb265b0d37b6f8a35ff7318a16eb083db666369')]}, text='LlamaIndex Newsletter 2024-07-16  LlamaIndex, Data Framework for LLM ApplicationsEnterpriseOpen sourceCommunityCareersBlog Talk to usEnterpriseOpen sourceCommunityCareersBlog Talk to us 2024 LlamaIndexLlamaIndex  Jul 16, 2024LlamaIndex Newsletter 2024-07-16Hello, Llama Family! Welcome to this weeks edition of the LlamaIndex newsletter! Were thrilled to share some exciting updates about our products, the implementation of GraphRAG, demos that have achieved over $1M in ARR, extensive guides, in-depth tutorials, and hackathons.Before we get into the details of our newsletter, were thrilled to share the beta launch of LlamaCloud. This new data processing layer boosts RAG workflows with sophisticated parsing, indexing, and retrieval functions. Alongside this, were also introducing LlamaTrace in partnership with Arize AI, which provides unmatched tracing, observability, and evaluation capabilities for LLM application workflows.Signup here: cloud.llamaindex.ai The highlights:LlamaCloud Launch: Weve launched the beta release of LlamaCloud, a data processing layer designed to enhance RAG workflows with state-of-the-art parsing, indexing, and retrieval capabilities. Blogpost, Tweet.LlamaTrace Launch: In collaboration with Arize AI, weve introduced LlamaTrace, offering unmatched tracing, observability, and evaluation capabilities for LLM application workflows. It features detailed call stack tracing, one-click setup through LlamaIndex, and seamless integration with LlamaCloud. Blogpost, Tweet.GraphRAG Implementation: Implementation of GraphRAG with LlamaIndex, focusing on graph generation, community building, summaries, and community-based retrieval to improve answer aggregation. Notebook, Tweet.Redis Queue Integration with Llama-Agents: We have integrated Redis Queue with llama-agents to boost coordination and communication in multi-agent workflows, ensuring robust performance. Notebook, Tweet. Feature Releases and Enhancements:We have launched the beta release of LlamaCloud, a data processing layer that enhances RAG workflows with advanced parsing, indexing, and retrieval capabilities. Blogpost, Tweet.We have launched an implementation[beta] of GraphRAG concepts with LlamaIndex focussing on graph generation, building communities and community summaries, and community-based retrieval to aggregate answers from summaries. Notebook, Tweet.We have integrated Redis Queue with llama-agents to enhance coordination in multi-agent workflows, allowing for robust communication. Notebook, Tweet.We have introduced LlamaTrace in collaboration with Arize AI, offering unparalleled tracing, observability, and evaluation capabilities for LLM application workflows. LlamaTrace stands out for its detailed tracing, which logs the entire call stack, one-click setup through LlamaIndex, and seamless integration with LlamaCloud for easy access and authentication. Blogpost, Tweet.We have integrated NebulaGraph with LlamaIndex, enhancing PropertyGraph capabilities with sophisticated extractors, customizable properties on nodes and edges, and advanced retrieval options. Docs, Tweet. Demos:Lyzrai has achieved over $1M ARR using LlamaIndex! This full-stack autonomous AI agent framework enhances AI sales and marketing functions with LlamaIndexs data connectors and RAG capabilities, boasting rapid revenue growth, high accuracy, and customer satisfaction. Guides:Guide to Multi-Modal RAG for Document Processing that introduces a multi-modal RAG architecture using LlamaParse, LlamaIndex, and GPT-4o, designed to handle complex slide decks. Tweet.Guide to using LlamaParse and GPT-4o for Financial Report RAG to to effectively parse and synthesize complex financial documents, enhancing clarity and accuracy in data analysis.Guide to Building Agentic RAG with Llama3: Explore our comprehensive cookbooks, created in collaboration with AI at Meta, featuring advanced techniques from routing and tool use to constructing complex agent reasoning loops and multi-document agents using purely local models like Llama3. Tutorials:1LittleCoders video tutorial demonstrates how to deploy self-hosted llama-agents using Arcee AI, MistralAI, and Ollama, including setup, local model integration, and tool development.kingzzms tutorial on using LlamaIndex to build advanced RAG flows, detailing how to compose and visualize each step from basic retrieval and prompting to advanced techniques and evaluation with RAGAS.Mervin Praisons tutorial on using llama-agents, detailing the frameworks purpose, a step-by-step setup guide for multi-agent services, and how it stands out from other frameworks. Events:Join our online hackathon this Friday, 19th, to build AI apps with Llama 3 from Meta and win cash, credits, and prizes from us and our co-hosts TogetherAI, Milvus, and LablabAI.LlamaIndexBlogPartnersCareersContactStatusEnterpriseLlamaCloudLlamaParseOpen SourcePython packagePython docsTypeScript packageTypeScript docsLlamaHubGitHubCommunityNewsletterDiscordTwitter/XLinkedInYouTubeStarter projectscreate-llamaSEC InsightsChat LlamaIndexLlamaBotRAG CLI 2024 LlamaIndexPrivacy NoticeTerms of Service', mimetype='text/plain', start_char_idx=0, end_char_idx=5110, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nodes[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XicyHUr-2KpH"
      },
      "source": [
        "AutoMergingRetriever, which looks at a set of leaf nodes and recursively \"merges\" subsets of leaf nodes that reference a parent node beyond a given threshold. This allows us to consolidate potentially disparate, smaller contexts into a larger context that might help synthesis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMoI_yJQ1wJc",
        "outputId": "d5f44f62-7b98-4143-a8ca-6de66f5f2ca5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "482"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from llama_index.core.retrievers import AutoMergingRetriever\n",
        "from llama_index.core.node_parser import get_leaf_nodes, get_root_nodes\n",
        "leaf_nodes = get_leaf_nodes(nodes)\n",
        "len(leaf_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ve7rGU2K3Y9_",
        "outputId": "78db8d9f-9d00-414f-dbe6-4f33be5d976a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "184"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "root_nodes = get_root_nodes(nodes)\n",
        "len(root_nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3LPG5n4769m"
      },
      "source": [
        "We then load these nodes into storage. The leaf nodes are indexed and retrieved via a vector store - these are the nodes that will first be directly retrieved via similarity search. The other nodes will be retrieved from a docstore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "gBMOj-Hs3fz0"
      },
      "outputs": [],
      "source": [
        "## Load index into vector index\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core import StorageContext\n",
        "\n",
        "docstore = SimpleDocumentStore()\n",
        "\n",
        "# insert nodes into docstore\n",
        "docstore.add_documents(nodes)\n",
        "\n",
        "# define storage context (will include vector store by default too)\n",
        "storage_context = StorageContext.from_defaults(docstore=docstore)\n",
        "\n",
        "# We then define a VectorStoreIndex containing just the leaf-level nodes.\n",
        "base_index = VectorStoreIndex(\n",
        "    leaf_nodes,\n",
        "    storage_context=storage_context,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5ilGsIV3kjJ",
        "outputId": "65580427-6a08-4420-f62d-6903412f767b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Merging 2 nodes into parent node.\n",
            "> Parent node id: 4d7dabcf-ee2e-41af-90a1-3f428d597844.\n",
            "> Parent node text: Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems  Ll...\n",
            "\n",
            "> Merging 1 nodes into parent node.\n",
            "> Parent node id: e81e093e-bf96-419a-9acd-aea6a0265a5d.\n",
            "> Parent node text: His name is Peter.\")] index = VectorStoreIndex.from_documents(docs) # Define a query rewrite agen...\n",
            "\n",
            "> Merging 1 nodes into parent node.\n",
            "> Parent node id: 80ab5c6d-b8f9-46fa-aac6-7f1deec68513.\n",
            "> Parent node text: LlamaIndex Newsletter 2024-07-02  LlamaIndex, Data Framework for LLM ApplicationsEnterpriseOpen s...\n",
            "\n",
            "> Merging 2 nodes into parent node.\n",
            "> Parent node id: df506443-a581-4826-9d00-63229ca12281.\n",
            "> Parent node text: Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems  Ll...\n",
            "\n",
            "> Merging 1 nodes into parent node.\n",
            "> Parent node id: 71a33e83-9fdc-4ac6-a397-c046a2e638d4.\n",
            "> Parent node text: LlamaIndex Newsletter 2024-07-02  LlamaIndex, Data Framework for LLM ApplicationsEnterpriseOpen s...\n",
            "\n",
            "4\n",
            "6\n"
          ]
        }
      ],
      "source": [
        "base_retriever = base_index.as_retriever(similarity_top_k=6) # Without Automerging\n",
        "retriever = AutoMergingRetriever(base_retriever, storage_context, verbose=True)\n",
        "\n",
        "query_str = (\"What are key features of llama-agents?\")\n",
        "\n",
        "nodes = retriever.retrieve(query_str)\n",
        "base_nodes = base_retriever.retrieve(query_str) # Without Automerging\n",
        "print(len(nodes)) #combines closely related or overlapping information into single, more comprehensive nodes. So may be fewer nodes\n",
        "print(len(base_nodes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "r6_NRKK18LcS",
        "outputId": "48920464-a6e2-43db-c78e-6f49f536be91"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Node ID:** df506443-a581-4826-9d00-63229ca12281<br>**Similarity:** 0.6661840085211621<br>**Text:** Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems  LlamaIndex, Data Framework for LLM ApplicationsEnterpriseOpen sourceCommunityCareersBlog Talk to usEnterpriseOpen sourceCommunityCareersBlog Talk to us 2024 LlamaIndexLlamaIndex  Jun 26, 2024Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI SystemsAgentsWe're excited to announce the alpha release of llama-agents, a new open-source framework designed to simplify the process of building, iterating, and deploying multi-agent AI systems and turn your agents into production microservices. Whether you're working on complex question-answering systems, collaborative AI assistants, or distributed AI workflows, llama-agents provides the tools and structure you need to bring your ideas to life.Key Features of llama-agentsDistributed Service Oriented Architecture: every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks.Communication via standardized API interfaces: interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue.Define agentic and explicit orchestration flows: developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an agentic orchestrator that decides which agents are relevant to the task.Ease of deployment: launch, scale and monitor each agent and your control plane independently.Scalability and resource management: use our built-in observability tools to monitor the quality and performance of the system and each individual agent serviceLet's dive into how you can start using llama-agents to build your own multi-agent systems.Getting Started with llama-agentsFirst, install the framework using pip:pip install llama-agents llama-index-agent-openaiBasic System SetupHere's a simple example of how to set up a basic multi-agent system using llama-agents. First well bring in our dependencies and set up our control plane, which contains our LLM-powered orchestratorimport dotenv dotenv.load_dotenv() # our .env file defines OPENAI_API_KEY from llama_agents import ( AgentService, ControlPlaneServer, SimpleMessageQueue, AgentOrchestrator, ) from llama_index.core.agent import FunctionCallingAgentWorker from llama_index.core.tools import FunctionTool from llama_index.llms.openai import OpenAI import logging # turn on logging so we can see the system working logging.getLogger(\"llama_agents\").setLevel(logging.INFO) # Set up the message queue and control plane message_queue = SimpleMessageQueue() control_plane = ControlPlaneServer( message_queue=message_queue, orchestrator=AgentOrchestrator(llm=OpenAI()), )Next we create our tools using LlamaIndexs existing abstractions, provide those tools to an agent, and turn that agent into an independent microservice:# create a tool def get_the_secret_fact() -> str: \"\"\"Returns the secret fact.\"\"\" return \"The secret fact is: A baby llama is called a 'Cria'.\" tool = FunctionTool.from_defaults(fn=get_the_secret_fact) # Define an agent worker = FunctionCallingAgentWorker.from_tools([tool], llm=OpenAI()) agent = worker.as_agent() # Create an agent service agent_service = AgentService( agent=agent, message_queue=message_queue, description=\"General purpose assistant\", service_name=\"assistant\", )Finally we launch the service and the control plane. Note that here were using a helper function to run a single query through the system and then exit; next well show how to deploy this to production.# Set up the launcher for local testing from llama_agents import LocalLauncher launcher = LocalLauncher( [agent_service], control_plane, message_queue, ) # Run a single query through the system result = launcher.launch_single(\"What's the secret fact?\") print(result)Deploying Your Multi-Agent SystemOnce you've tested your system locally, you can deploy it as a set of services for real production use. Here's how you might set that up. This is similar to the previous example, but weve added a second agent service and were using a different launcher. Lets bring in our dependencies and set up our control plane again:import dotenv dotenv.load_dotenv() from llama_agents import ( AgentService, AgentOrchestrator, ControlPlaneServer, SimpleMessageQueue, ) from llama_index.core.agent import FunctionCallingAgentWorker from llama_index.core.tools import FunctionTool from llama_index.llms.openai import OpenAI import logging # change logging level to enable or disable more verbose logging logging.getLogger(\"llama_agents\").setLevel(logging.INFO) # create our multi-agent framework components message_queue = SimpleMessageQueue() control_plane = ControlPlaneServer( message_queue=message_queue, orchestrator=AgentOrchestrator(llm=OpenAI()), )Then as before we create a tool and an agent, though this time well add a second agent:# create a tool def get_the_secret_fact() -> str: \"\"\"Returns the secret fact.\"\"\" return \"The secret fact is: A baby llama is called a 'Cria'.\" tool = FunctionTool.from_defaults(fn=get_the_secret_fact) # create our agents worker1 = FunctionCallingAgentWorker.from_tools([tool], llm=OpenAI()) worker2 = FunctionCallingAgentWorker.from_tools([], llm=OpenAI()) agent1 = worker1.as_agent() agent2 = worker2.as_agent()We turn those agents into services:agent_server_1 = AgentService( agent=agent1, message_queue=message_queue, description=\"Useful for getting the secret fact.\", service_name=\"secret_fact_agent\", host=\"localhost\", port=8003 ) agent_server_2 = AgentService( agent=agent2, message_queue=message_queue, description=\"Useful for getting random dumb facts.\", service_name=\"dumb_fact_agent\", host=\"localhost\", port=8004 )And finally we launch each service as an independent agent. Here were doing them all from a single script, but each of these could be a totally separate service, launched and scaled independently:from llama_agents import ServerLauncher, CallableMessageConsumer # Additional human consumer def handle_result(message) -> None: print(f\"Got result:\", message.data) # the final result is published to a \"human\" consumer # so we define one to handle it! human_consumer = CallableMessageConsumer( handler=handle_result, message_type=\"human\" ) # Define Launcher launcher = ServerLauncher( [agent_server_1, agent_server_2], control_plane, message_queue, additional_consumers=[human_consumer] ) launcher.launch_servers()Real-time monitoringOne of the coolest debugging features of our multi-agent system is our agent monitor, which is built right in. You launch it like this:llama-agents monitor --control-plane-url http://127.0.0.1:8000Once launched, you get an intuitive, point-and-click terminal application. You can see both of the agents running, and at the bottom you can inject a task like the query What is the secret fact? Youll get a job ID which you can then click on to see your results:Building a Query Rewriting RAG SystemLet's look at a more complex example: a Query Rewriting RAG system. This system will rewrite user queries to improve retrieval, then use the rewritten query to perform RAG over a document.This example demonstrates how to create a more sophisticated system that combines query rewriting with RAG to improve question-answering capabilities. See this notebook for a fuller explanation of whats going on.import dotenv dotenv.load_dotenv() # our .env defines OPENAI_API_KEY from llama_index.core import VectorStoreIndex, Document from llama_index.core.agent import FnAgentWorker from llama_index.core import PromptTemplate from llama_index.core.query_pipeline import QueryPipeline from llama_index.core.query_engine import RetrieverQueryEngine from llama_agents import ( AgentService, ControlPlaneServer, SimpleMessageQueue, PipelineOrchestrator, ServiceComponent, ) from llama_agents.launchers import LocalLauncher from llama_index.llms.openai import OpenAI import logging # change logging level to enable or disable more verbose logging logging.getLogger(\"llama_agents\").setLevel(logging.INFO) # Load and index your document docs = [Document(text=\"The rabbit is a small mammal with long ears and a fluffy tail. His name is Peter.\")] index = VectorStoreIndex.from_documents(docs) # Define a query rewrite agent HYDE_PROMPT_STR = ( \"Please rewrite the following query to include more detail:\\n{query_str}\\n\" ) HYDE_PROMPT_TMPL = PromptTemplate(HYDE_PROMPT_STR) def run_hyde_fn(state): prompt_tmpl, llm, input_str = ( state[\"prompt_tmpl\"], state[\"llm\"], state[\"__task__\"].input, ) qp = QueryPipeline(chain=[prompt_tmpl, llm]) output = qp.run(query_str=input_str) state[\"__output__\"] = str(output) return state, True hyde_agent = FnAgentWorker( fn=run_hyde_fn, initial_state={\"prompt_tmpl\": HYDE_PROMPT_TMPL, \"llm\": OpenAI()} ).as_agent() # Define a RAG agent def run_rag_fn(state): retriever, llm, input_str = ( state[\"retriever\"], state[\"llm\"], state[\"__task__\"].input, ) query_engine = RetrieverQueryEngine.from_args(retriever, llm=llm) response = query_engine.query(input_str) state[\"__output__\"] = str(response) return state, True rag_agent = FnAgentWorker( fn=run_rag_fn, initial_state={\"retriever\": index.as_retriever(), \"llm\": OpenAI()} ).as_agent() # Set up the multi-agent system message_queue = SimpleMessageQueue() query_rewrite_service = AgentService( agent=hyde_agent, message_queue=message_queue, description=\"Query rewriting service\", service_name=\"query_rewrite\", ) rag_service = AgentService( agent=rag_agent, message_queue=message_queue, description=\"RAG service\", service_name=\"rag\", ) # Create the pipeline pipeline = QueryPipeline(chain=[ ServiceComponent.from_service_definition(query_rewrite_service), ServiceComponent.from_service_definition(rag_service), ]) orchestrator = PipelineOrchestrator(pipeline) control_plane = ControlPlaneServer( message_queue=message_queue, orchestrator=orchestrator, ) # Set up the launcher launcher = LocalLauncher( [query_rewrite_servi...<br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Node ID:** 3058ab62-4cec-4b57-a570-cbc63c07b0be<br>**Similarity:** 0.63911602171461<br>**Text:** For instance, you may want to define a specific workflow over text-to-SQL with our NLStructStoreQueryEngine (constrained), or a router module to decide between semantic search or summarization (less constrained), or use our SubQuestionQueryEngine to decompose a question among sub-documents (even less constrained).By default, agent loops are unconstrained, and can theoretically reason over any set of tools that you give them. This means that you can get out-of-the-box advanced search/retrieval capabilities  for instance, in our OpenAI cookbook we show that you can get joint text-to-SQL capabilities by simply providing a SQL query engine and Vector Store Query engine as tools. But on the other hand, agents built in this fashion can be quite unreliable (see our blog post for more insights). If you are using agents for search/retrieval, be mindful of the 1) LLM you pick, and the 2) set of tools you pick too.How are LlamaIndex data agents different than existing agent frameworks (LangChain, Hugging Face, etc.)?Most of these core concepts are not new. Our overall design has taken inspiration from popular tools and frameworks for building agents. But in our data agents design, weve tried our best to answer the following key questions well:How do we effectively index/query and retrieve data beforehand?How do we effectively index/query and retrieve data on the fly?How do we design API interfaces for read/writes that are simultaneously rich (can take in structured inputs), but also easy for agents to understand?How do we properly get sources in citations?Our goal with data agents is to create automated knowledge workers that can reason over and interact with data. Our core toolkit provides the foundations for properly indexing, retrieving, and querying data  these can be easily integrated as tools. We provide some additional tool abstractions to handle the cases where you want to cache API outputs on the fly (see above). Finally, we provide principled tool abstractions and design principles so that agents can interface with external services in a structured manner.Can I use Tools with LangChain agents? You can easily use any of our tools with LangChain agents as well.tools = tool_spec.to_tool_list() langchain_tools = [t.to_langchain_tool() for t in tools]See our tools usage guide for more details!ConclusionIn summary, today we launched two key items: Data Agent components (incl. agent reasoning loop and tool abstractions) and the LlamaHub Tool repository.ResourcesWeve written a comprehensive section in the docs  take a look here: https://gpt-index.readthedocs.io/en/latest/core_modules/agent_modules/agents/root.htmlTake a look at our LlamaHub Tools section: https://llamahub.ai/Notebook Tutorials for LlamaHub Tools: https://github.com/emptycrown/llama-hub/tree/main/llama_hub/tools/notebooksIf you have questions, please hop on our Discord: https://discord.gg/dGcwcsnxhURelated articlesBuilding a multi-agent concierge system2024-07-17Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems2024-06-26Automate online tasks with MultiOn and LlamaIndex2024-05-23Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations2024-03-19LlamaIndexBlogPartnersCareersContactStatusEnterpriseLlamaCloudLlamaParseOpen SourcePython packagePython docsTypeScript packageTypeScript docsLlamaHubGitHubCommunityNewsletterDiscordTwitter/XLinkedInYouTubeStarter projectscreate-llamaSEC InsightsChat LlamaIndexLlamaBotRAG CLI 2024 LlamaIndexPrivacy NoticeTerms of Service<br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Node ID:** 3b9739f3-af80-476a-a621-a69d4e1a5540<br>**Similarity:** 0.6332048798838363<br>**Text:** Data Agents  LlamaIndex, Data Framework for LLM ApplicationsEnterpriseOpen sourceCommunityCareersBlog Talk to usEnterpriseOpen sourceCommunityCareersBlog Talk to us 2024 LlamaIndexJerry Liu  Jul 12, 2023Data AgentsLlamaindexNLPAIAgentsDataToday were incredibly excited to announce the launch of a big new capability within LlamaIndex: Data Agents.Data Agents are LLM-powered knowledge workers that can intelligently perform various tasks over your data, in both a read and write function. They are capable of the following:Perform automated search and retrieval over different types of data  unstructured, semi-structured, and structured.Calling any external service API in a structured fashion. They can either process the response immediately, or index/cache this data for future use.Storing conversation history.Using all of the above to fulfill both simple and complex data tasks.Weve worked hard to provide abstractions, services, and guides on both the agents side and tools side in order to build data agents. Todays launch consists of the following key components:General Agent/Tool Abstractions: a set of abstractions to build agent loops, and to have those loops interact with tools according to a structured API definition.LlamaHub Tool Repository: A brand-new section within LlamaHub that consists of 15+ Tools (e.g. Google Calendar, Notion, SQL, OpenAPI) that can be connected. Opening to community contributions!See below for full details. We show you how to build a Gmail agent thats able to automatically create/send emails in <10 lines of code!ContextOur core mission at LlamaIndex is to unlock the full capabilities of LLMs over your external sources of data. It provides a set of tools to both define state (how to parse/structure your data), and compute (how to query your data). Up until now, our framework has primarily focused on search and retrieval use case. We have an incredible suite of tools and capabilities that not only allow you to create the basic RAG stack around a vector database + top-k retrieval, but also offer much greater functionality beyond that.A lot of that technology used to lie in our query engines. Our goal was to increase the capability of query engines to answer a wide range of different queries. In order to do this, we had to improve the reasoning capabilities of these query engines. As a result some of our existing query capabilities contain agent-like components: we have query engines capable of chain-of-thought reasoning, query decomposition, and routing. In the process, users had the option of choosing from a spectrum of query engines that had more constrained reasoning capabilities to less constrained capabilities.But there was a huge opportunity for LLMs to have an even richer set of interactions with data; they should be capable of general reasoning over any set of tools, whether from a database or an API. They should also be capable of both read and write capabilities  the ability to not only understand state but also modify it. As a result they should be able to do more than search and retrieval from a static knowledge source.Some existing services, toolkits, and research papers have already demonstrated the possibilities of LLM-powered agents that can interact with the external environment. Using these existing approaches as inspiration, we saw an opportunity to build a principled series of abstractions enabling anyone to build knowledge workers over their data.Core Components of Data AgentsBuilding a data agent requires the following core components:A reasoning loopTool abstractionsAt a high-level, a data agent is provided with a set of APIs, or Tools, to interact with. These APIs can return information about the world, or perform an action that modifies state. Each Tool exposes a request/response interface. The request is a set of structured parameters, and the response can be any format (at least conceptually, in most cases the response here is a text string of some form).Given an input task, the data agent uses a reasoning loop to decide which tools to use, in which sequence, and the parameters to call each tool.<br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Node ID:** 71a33e83-9fdc-4ac6-a397-c046a2e638d4<br>**Similarity:** 0.6283408263769629<br>**Text:** LlamaIndex Newsletter 2024-07-02  LlamaIndex, Data Framework for LLM ApplicationsEnterpriseOpen sourceCommunityCareersBlog Talk to usEnterpriseOpen sourceCommunityCareersBlog Talk to us 2024 LlamaIndexLlamaIndex  Jul 2, 2024LlamaIndex Newsletter 2024-07-02Hello, Llama enthusiasts! Welcome to this weeks edition of the LlamaIndex newsletter! In this issue, were excited to bring you exciting updates about llama-agents, live demos, extensive guides, and in-depth tutorials to enhance your understanding of our tools.Before moving into our newsletter, we have an exciting update on our enterprise offerings. We are thrilled to announce the waitlist release of LlamaCloud, our fully-managed ingestion service. Sign up now if youre eager to collaborate and build LLM applications with LlamaCloud. The highlights:Launched Llama-Agents Framework: Our new alpha-release, llama-agents, enables multi-agent AI systems for production with a distributed architecture, HTTP API communication, and agentic orchestration. Its designed for easy deployment, scalability, and observability. Blogpost, Tweet.create-llama Integrated with LlamaCloud: Streamline your LLM application data pipelines with create-llama, now integrated with LlamaCloud for faster setup and efficient system maintenance. Tweet. Feature Releases and Enhancements:We have launched llama-agents - new alpha-release framework that enables multi-agent AI systems to go into production. It features a distributed, service-oriented architecture, communication through standard HTTP APIs, agentic orchestration of flows, and is designed for easy deployment, scalability, and observability. Blogpost, Tweet.create-llama is now integrated with LlamaCloud to streamline the setup and management of data pipelines for LLM applications, providing a fast and efficient way to deploy and maintain these systems. Tweet.We have integrated with DSPy for Optimized RAG by utilizing DSPys optimization capabilities with LlamaIndexs data tools to enhance your query pipelines, optimize prompts, or repurpose DSPy predictors. Cookbook, Tweet. Demos:Automating Code Reviews, project by Composio with LlamaIndex automates code reviews using an AI agent in under 100 lines of code that monitors GitHub PRs, reviews them immediately upon creation, and posts feedback directly to your Slack channel. Codebase. Guides:Guide to Building an Agentic RAG Service with our comprehensive notebook that walks you through creating vector indexes, transforming them into query engines, turning each engine into a tool, providing these tools to agents, and launching the agents as services.Guide to AI Agents with LlamaIndex: Andreis comprehensive workshop from Gen AI Philippines, showcasing LLM applications through LlamaIndex. This beginner-friendly session covers topics from RAG to multi-hop agents. Video, Notebook. Tutorials:Kingzzms tutorial on crafting a custom hybrid retriever using LlamaIndexs flexible abstractions. This tutorial teaches you how to integrate full text and dense search capabilities from Elastic, and how to write your own reciprocal rank fusion function for optimal retrieval strategy.Jeffs tutorial on which outlines the essential tools needed to construct a report generator using a ReAct agent. Learn how to integrate a RAG tool over guideline documents, a web search tool, and a report generation tool that converts markdown text into PDFs.1littlecoders tutorial on llama-agents provides a detailed introduction to transforming multi-agent systems into microservices for production, including setup examples and a walkthrough of the architecture involving the control plane, message queue, and agent services using LlamaIndex abstractions.Mervin Praisons tutorial on the llama-agents framework provides a concise guide to setting up agent services, from notebook synchronization to server-client interactions, complete with over 10 practical examples.LlamaIndexBlogPartnersCareersContactStatusEnterpriseLlamaCloudLlamaParseOpen SourcePython packagePython docsTypeScript packageTypeScript docsLlamaHubGitHubCommunityNewsletterDiscordTwitter/XLinkedInYouTubeStarter projectscreate-llamaSEC InsightsChat LlamaIndexLlamaBotRAG CLI 2024 LlamaIndexPrivacy NoticeTerms of Service<br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from llama_index.core.response.notebook_utils import display_source_node\n",
        "\n",
        "for node in nodes:\n",
        "    display_source_node(node, source_length=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "83acI4zA8n9z",
        "outputId": "49270782-2717-4ce1-9554-da9e50693c9d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Node ID:** ec696876-3d24-443a-8d8b-82043e01c9ce<br>**Similarity:** 0.6919124729110046<br>**Text:** Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems  LlamaIndex, Data Framework for LLM ApplicationsEnterpriseOpen sourceCommunityCareersBlog Talk to usEnterpriseOpen sourceCommunityCareersBlog Talk to us 2024 LlamaIndexLlamaIndex  Jun 26, 2024Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI SystemsAgentsWe're excited to announce the alpha release of llama-agents, a new open-source framework designed to simplify the process of building, iterating, and deploying multi-agent AI systems and turn your agents into production microservices. Whether you're working on complex question-answering systems, collaborative AI assistants, or distributed AI workflows, llama-agents provides the tools and structure you need to bring your ideas to life.Key Features of llama-agentsDistributed Service Oriented Architecture: every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks.Communication via standardized API interfaces: interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue.Define agentic and explicit orchestration flows: developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an agentic orchestrator that decides which agents are relevant to the task.Ease of deployment: launch, scale and monitor each agent and your control plane independently.Scalability and resource management: use our built-in observability tools to monitor the quality and performance of the system and each individual agent serviceLet's dive into how you can start using llama-agents to build your own multi-agent systems.Getting Started with llama-agentsFirst, install the framework using pip:pip install llama-agents llama-index-agent-openaiBasic System SetupHere's a simple example of how to set up a basic multi-agent system using llama-agents. First well bring in our dependencies and set up our control plane, which contains our LLM-powered orchestratorimport dotenv dotenv.load_dotenv() # our .env file defines OPENAI_API_KEY from llama_agents import ( AgentService, ControlPlaneServer, SimpleMessageQueue, AgentOrchestrator, ) from llama_index.core.agent import FunctionCallingAgentWorker from llama_index.core.tools import FunctionTool from llama_index.llms.openai import OpenAI import logging # turn on logging so we can see the system working logging.getLogger(\"llama_agents\").setLevel(logging.INFO) # Set up the message queue and control plane message_queue = SimpleMessageQueue() control_plane = ControlPlaneServer( message_queue=message_queue, orchestrator=AgentOrchestrator(llm=OpenAI()), )Next we create our tools using LlamaIndexs existing abstractions, provide those tools to an agent, and turn that agent into an independent microservice:# create a tool def get_the_secret_fact() -> str: \"\"\"Returns the secret fact.\"\"\" return \"The secret fact is: A baby llama is called a 'Cria'.\" tool = FunctionTool.from_defaults(fn=get_the_secret_fact) # Define an agent worker = FunctionCallingAgentWorker.from_tools([tool], llm=OpenAI()) agent = worker.as_agent() # Create an agent service agent_service = AgentService( agent=agent, message_queue=message_queue, description=\"General purpose assistant\", service_name=\"assistant\", )Finally we launch the service and the control plane. Note that here were using a helper function to run a single query through the system and then exit; next well show how to deploy this to production.# Set up the launcher for local testing from llama_agents import LocalLauncher launcher = LocalLauncher( [agent_service], control_plane, message_queue, ) # Run a single query through the system result = launcher.launch_single(\"What's the secret fact?\") print(result)Deploying Your Multi-Agent SystemOnce you've tested your system locally, you can deploy it as a set of services for real production use. Here's how you might set that up. This is similar to the previous example, but weve added a second agent service and were using a different launcher.<br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Node ID:** 6ba7db28-55e5-46b8-aec2-3fa5ea29d495<br>**Similarity:** 0.6707382559525482<br>**Text:** His name is Peter.\")] index = VectorStoreIndex.from_documents(docs) # Define a query rewrite agent HYDE_PROMPT_STR = ( \"Please rewrite the following query to include more detail:\\n{query_str}\\n\" ) HYDE_PROMPT_TMPL = PromptTemplate(HYDE_PROMPT_STR) def run_hyde_fn(state): prompt_tmpl, llm, input_str = ( state[\"prompt_tmpl\"], state[\"llm\"], state[\"__task__\"].input, ) qp = QueryPipeline(chain=[prompt_tmpl, llm]) output = qp.run(query_str=input_str) state[\"__output__\"] = str(output) return state, True hyde_agent = FnAgentWorker( fn=run_hyde_fn, initial_state={\"prompt_tmpl\": HYDE_PROMPT_TMPL, \"llm\": OpenAI()} ).as_agent() # Define a RAG agent def run_rag_fn(state): retriever, llm, input_str = ( state[\"retriever\"], state[\"llm\"], state[\"__task__\"].input, ) query_engine = RetrieverQueryEngine.from_args(retriever, llm=llm) response = query_engine.query(input_str) state[\"__output__\"] = str(response) return state, True rag_agent = FnAgentWorker( fn=run_rag_fn, initial_state={\"retriever\": index.as_retriever(), \"llm\": OpenAI()} ).as_agent() # Set up the multi-agent system message_queue = SimpleMessageQueue() query_rewrite_service = AgentService( agent=hyde_agent, message_queue=message_queue, description=\"Query rewriting service\", service_name=\"query_rewrite\", ) rag_service = AgentService( agent=rag_agent, message_queue=message_queue, description=\"RAG service\", service_name=\"rag\", ) # Create the pipeline pipeline = QueryPipeline(chain=[ ServiceComponent.from_service_definition(query_rewrite_service), ServiceComponent.from_service_definition(rag_service), ]) orchestrator = PipelineOrchestrator(pipeline) control_plane = ControlPlaneServer( message_queue=message_queue, orchestrator=orchestrator, ) # Set up the launcher launcher = LocalLauncher( [query_rewrite_service, rag_service], control_plane, message_queue, ) # Run a query result = launcher.launch_single(\"Tell me about rabbits\") print(result)Public roadmapThis is an alpha release, meaning that wed love your feedback on features to better help you build multi-agent systems in production! Weve created a public roadmap showing where we plan to go from here. Were actively seeking public feedback on what works for you and what doesnt.Dive in!llama-agents provides a powerful, flexible framework for building complex multi-agent AI systems. Whether you're prototyping a new idea or scaling to production, llama-agents offers the tools you need to bring your AI vision to life. Check out the repo to learn more, especially our library of examples.We're excited to see what the community builds with llama-agents. Happy coding!Related articlesBuilding a multi-agent concierge system2024-07-17Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems2024-06-26Automate online tasks with MultiOn and LlamaIndex2024-05-23How to build LLM Agents in TypeScript with LlamaIndex.TS2024-02-08LlamaIndexBlogPartnersCareersContactStatusEnterpriseLlamaCloudLlamaParseOpen SourcePython packagePython docsTypeScript packageTypeScript docsLlamaHubGitHubCommunityNewsletterDiscordTwitter/XLinkedInYouTubeStarter projectscreate-llamaSEC InsightsChat LlamaIndexLlamaBotRAG CLI 2024 LlamaIndexPrivacy NoticeTerms of Service<br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Node ID:** 3058ab62-4cec-4b57-a570-cbc63c07b0be<br>**Similarity:** 0.63911602171461<br>**Text:** For instance, you may want to define a specific workflow over text-to-SQL with our NLStructStoreQueryEngine (constrained), or a router module to decide between semantic search or summarization (less constrained), or use our SubQuestionQueryEngine to decompose a question among sub-documents (even less constrained).By default, agent loops are unconstrained, and can theoretically reason over any set of tools that you give them. This means that you can get out-of-the-box advanced search/retrieval capabilities  for instance, in our OpenAI cookbook we show that you can get joint text-to-SQL capabilities by simply providing a SQL query engine and Vector Store Query engine as tools. But on the other hand, agents built in this fashion can be quite unreliable (see our blog post for more insights). If you are using agents for search/retrieval, be mindful of the 1) LLM you pick, and the 2) set of tools you pick too.How are LlamaIndex data agents different than existing agent frameworks (LangChain, Hugging Face, etc.)?Most of these core concepts are not new. Our overall design has taken inspiration from popular tools and frameworks for building agents. But in our data agents design, weve tried our best to answer the following key questions well:How do we effectively index/query and retrieve data beforehand?How do we effectively index/query and retrieve data on the fly?How do we design API interfaces for read/writes that are simultaneously rich (can take in structured inputs), but also easy for agents to understand?How do we properly get sources in citations?Our goal with data agents is to create automated knowledge workers that can reason over and interact with data. Our core toolkit provides the foundations for properly indexing, retrieving, and querying data  these can be easily integrated as tools. We provide some additional tool abstractions to handle the cases where you want to cache API outputs on the fly (see above). Finally, we provide principled tool abstractions and design principles so that agents can interface with external services in a structured manner.Can I use Tools with LangChain agents? You can easily use any of our tools with LangChain agents as well.tools = tool_spec.to_tool_list() langchain_tools = [t.to_langchain_tool() for t in tools]See our tools usage guide for more details!ConclusionIn summary, today we launched two key items: Data Agent components (incl. agent reasoning loop and tool abstractions) and the LlamaHub Tool repository.ResourcesWeve written a comprehensive section in the docs  take a look here: https://gpt-index.readthedocs.io/en/latest/core_modules/agent_modules/agents/root.htmlTake a look at our LlamaHub Tools section: https://llamahub.ai/Notebook Tutorials for LlamaHub Tools: https://github.com/emptycrown/llama-hub/tree/main/llama_hub/tools/notebooksIf you have questions, please hop on our Discord: https://discord.gg/dGcwcsnxhURelated articlesBuilding a multi-agent concierge system2024-07-17Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems2024-06-26Automate online tasks with MultiOn and LlamaIndex2024-05-23Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations2024-03-19LlamaIndexBlogPartnersCareersContactStatusEnterpriseLlamaCloudLlamaParseOpen SourcePython packagePython docsTypeScript packageTypeScript docsLlamaHubGitHubCommunityNewsletterDiscordTwitter/XLinkedInYouTubeStarter projectscreate-llamaSEC InsightsChat LlamaIndexLlamaBotRAG CLI 2024 LlamaIndexPrivacy NoticeTerms of Service<br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Node ID:** 3b9739f3-af80-476a-a621-a69d4e1a5540<br>**Similarity:** 0.6332048798838363<br>**Text:** Data Agents  LlamaIndex, Data Framework for LLM ApplicationsEnterpriseOpen sourceCommunityCareersBlog Talk to usEnterpriseOpen sourceCommunityCareersBlog Talk to us 2024 LlamaIndexJerry Liu  Jul 12, 2023Data AgentsLlamaindexNLPAIAgentsDataToday were incredibly excited to announce the launch of a big new capability within LlamaIndex: Data Agents.Data Agents are LLM-powered knowledge workers that can intelligently perform various tasks over your data, in both a read and write function. They are capable of the following:Perform automated search and retrieval over different types of data  unstructured, semi-structured, and structured.Calling any external service API in a structured fashion. They can either process the response immediately, or index/cache this data for future use.Storing conversation history.Using all of the above to fulfill both simple and complex data tasks.Weve worked hard to provide abstractions, services, and guides on both the agents side and tools side in order to build data agents. Todays launch consists of the following key components:General Agent/Tool Abstractions: a set of abstractions to build agent loops, and to have those loops interact with tools according to a structured API definition.LlamaHub Tool Repository: A brand-new section within LlamaHub that consists of 15+ Tools (e.g. Google Calendar, Notion, SQL, OpenAPI) that can be connected. Opening to community contributions!See below for full details. We show you how to build a Gmail agent thats able to automatically create/send emails in <10 lines of code!ContextOur core mission at LlamaIndex is to unlock the full capabilities of LLMs over your external sources of data. It provides a set of tools to both define state (how to parse/structure your data), and compute (how to query your data). Up until now, our framework has primarily focused on search and retrieval use case. We have an incredible suite of tools and capabilities that not only allow you to create the basic RAG stack around a vector database + top-k retrieval, but also offer much greater functionality beyond that.A lot of that technology used to lie in our query engines. Our goal was to increase the capability of query engines to answer a wide range of different queries. In order to do this, we had to improve the reasoning capabilities of these query engines. As a result some of our existing query capabilities contain agent-like components: we have query engines capable of chain-of-thought reasoning, query decomposition, and routing. In the process, users had the option of choosing from a spectrum of query engines that had more constrained reasoning capabilities to less constrained capabilities.But there was a huge opportunity for LLMs to have an even richer set of interactions with data; they should be capable of general reasoning over any set of tools, whether from a database or an API. They should also be capable of both read and write capabilities  the ability to not only understand state but also modify it. As a result they should be able to do more than search and retrieval from a static knowledge source.Some existing services, toolkits, and research papers have already demonstrated the possibilities of LLM-powered agents that can interact with the external environment. Using these existing approaches as inspiration, we saw an opportunity to build a principled series of abstractions enabling anyone to build knowledge workers over their data.Core Components of Data AgentsBuilding a data agent requires the following core components:A reasoning loopTool abstractionsAt a high-level, a data agent is provided with a set of APIs, or Tools, to interact with. These APIs can return information about the world, or perform an action that modifies state. Each Tool exposes a request/response interface. The request is a set of structured parameters, and the response can be any format (at least conceptually, in most cases the response here is a text string of some form).Given an input task, the data agent uses a reasoning loop to decide which tools to use, in which sequence, and the parameters to call each tool.<br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Node ID:** 87691d04-b40e-44c3-abfc-df4e157d5bcb<br>**Similarity:** 0.6313470492685472<br>**Text:** Lets bring in our dependencies and set up our control plane again:import dotenv dotenv.load_dotenv() from llama_agents import ( AgentService, AgentOrchestrator, ControlPlaneServer, SimpleMessageQueue, ) from llama_index.core.agent import FunctionCallingAgentWorker from llama_index.core.tools import FunctionTool from llama_index.llms.openai import OpenAI import logging # change logging level to enable or disable more verbose logging logging.getLogger(\"llama_agents\").setLevel(logging.INFO) # create our multi-agent framework components message_queue = SimpleMessageQueue() control_plane = ControlPlaneServer( message_queue=message_queue, orchestrator=AgentOrchestrator(llm=OpenAI()), )Then as before we create a tool and an agent, though this time well add a second agent:# create a tool def get_the_secret_fact() -> str: \"\"\"Returns the secret fact.\"\"\" return \"The secret fact is: A baby llama is called a 'Cria'.\" tool = FunctionTool.from_defaults(fn=get_the_secret_fact) # create our agents worker1 = FunctionCallingAgentWorker.from_tools([tool], llm=OpenAI()) worker2 = FunctionCallingAgentWorker.from_tools([], llm=OpenAI()) agent1 = worker1.as_agent() agent2 = worker2.as_agent()We turn those agents into services:agent_server_1 = AgentService( agent=agent1, message_queue=message_queue, description=\"Useful for getting the secret fact.\", service_name=\"secret_fact_agent\", host=\"localhost\", port=8003 ) agent_server_2 = AgentService( agent=agent2, message_queue=message_queue, description=\"Useful for getting random dumb facts.\", service_name=\"dumb_fact_agent\", host=\"localhost\", port=8004 )And finally we launch each service as an independent agent. Here were doing them all from a single script, but each of these could be a totally separate service, launched and scaled independently:from llama_agents import ServerLauncher, CallableMessageConsumer # Additional human consumer def handle_result(message) -> None: print(f\"Got result:\", message.data) # the final result is published to a \"human\" consumer # so we define one to handle it! human_consumer = CallableMessageConsumer( handler=handle_result, message_type=\"human\" ) # Define Launcher launcher = ServerLauncher( [agent_server_1, agent_server_2], control_plane, message_queue, additional_consumers=[human_consumer] ) launcher.launch_servers()Real-time monitoringOne of the coolest debugging features of our multi-agent system is our agent monitor, which is built right in. You launch it like this:llama-agents monitor --control-plane-url http://127.0.0.1:8000Once launched, you get an intuitive, point-and-click terminal application. You can see both of the agents running, and at the bottom you can inject a task like the query What is the secret fact? Youll get a job ID which you can then click on to see your results:Building a Query Rewriting RAG SystemLet's look at a more complex example: a Query Rewriting RAG system. This system will rewrite user queries to improve retrieval, then use the rewritten query to perform RAG over a document.This example demonstrates how to create a more sophisticated system that combines query rewriting with RAG to improve question-answering capabilities. See this notebook for a fuller explanation of whats going on.import dotenv dotenv.load_dotenv() # our .env defines OPENAI_API_KEY from llama_index.core import VectorStoreIndex, Document from llama_index.core.agent import FnAgentWorker from llama_index.core import PromptTemplate from llama_index.core.query_pipeline import QueryPipeline from llama_index.core.query_engine import RetrieverQueryEngine from llama_agents import ( AgentService, ControlPlaneServer, SimpleMessageQueue, PipelineOrchestrator, ServiceComponent, ) from llama_agents.launchers import LocalLauncher from llama_index.llms.openai import OpenAI import logging # change logging level to enable or disable more verbose logging logging.getLogger(\"llama_agents\").setLevel(logging.INFO) # Load and index your document docs = [Document(text=\"The rabbit is a small mammal with long ears and a fluffy tail. His name is Peter.\")]<br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Node ID:** 76606973-5bf0-47bd-9440-cd4989710607<br>**Similarity:** 0.6283408263769629<br>**Text:** LlamaIndex Newsletter 2024-07-02  LlamaIndex, Data Framework for LLM ApplicationsEnterpriseOpen sourceCommunityCareersBlog Talk to usEnterpriseOpen sourceCommunityCareersBlog Talk to us 2024 LlamaIndexLlamaIndex  Jul 2, 2024LlamaIndex Newsletter 2024-07-02Hello, Llama enthusiasts! Welcome to this weeks edition of the LlamaIndex newsletter! In this issue, were excited to bring you exciting updates about llama-agents, live demos, extensive guides, and in-depth tutorials to enhance your understanding of our tools.Before moving into our newsletter, we have an exciting update on our enterprise offerings. We are thrilled to announce the waitlist release of LlamaCloud, our fully-managed ingestion service. Sign up now if youre eager to collaborate and build LLM applications with LlamaCloud. The highlights:Launched Llama-Agents Framework: Our new alpha-release, llama-agents, enables multi-agent AI systems for production with a distributed architecture, HTTP API communication, and agentic orchestration. Its designed for easy deployment, scalability, and observability. Blogpost, Tweet.create-llama Integrated with LlamaCloud: Streamline your LLM application data pipelines with create-llama, now integrated with LlamaCloud for faster setup and efficient system maintenance. Tweet. Feature Releases and Enhancements:We have launched llama-agents - new alpha-release framework that enables multi-agent AI systems to go into production. It features a distributed, service-oriented architecture, communication through standard HTTP APIs, agentic orchestration of flows, and is designed for easy deployment, scalability, and observability. Blogpost, Tweet.create-llama is now integrated with LlamaCloud to streamline the setup and management of data pipelines for LLM applications, providing a fast and efficient way to deploy and maintain these systems. Tweet.We have integrated with DSPy for Optimized RAG by utilizing DSPys optimization capabilities with LlamaIndexs data tools to enhance your query pipelines, optimize prompts, or repurpose DSPy predictors. Cookbook, Tweet. Demos:Automating Code Reviews, project by Composio with LlamaIndex automates code reviews using an AI agent in under 100 lines of code that monitors GitHub PRs, reviews them immediately upon creation, and posts feedback directly to your Slack channel. Codebase. Guides:Guide to Building an Agentic RAG Service with our comprehensive notebook that walks you through creating vector indexes, transforming them into query engines, turning each engine into a tool, providing these tools to agents, and launching the agents as services.Guide to AI Agents with LlamaIndex: Andreis comprehensive workshop from Gen AI Philippines, showcasing LLM applications through LlamaIndex. This beginner-friendly session covers topics from RAG to multi-hop agents. Video, Notebook. Tutorials:Kingzzms tutorial on crafting a custom hybrid retriever using LlamaIndexs flexible abstractions. This tutorial teaches you how to integrate full text and dense search capabilities from Elastic, and how to write your own reciprocal rank fusion function for optimal retrieval strategy.Jeffs tutorial on which outlines the essential tools needed to construct a report generator using a ReAct agent. Learn how to integrate a RAG tool over guideline documents, a web search tool, and a report generation tool that converts markdown text into PDFs.1littlecoders tutorial on llama-agents provides a detailed introduction to transforming multi-agent systems into microservices for production, including setup examples and a walkthrough of the architecture involving the control plane, message queue, and agent services using LlamaIndex abstractions.Mervin Praisons tutorial on the llama-agents framework provides a concise guide to setting up agent services, from notebook synchronization to server-client interactions, complete with over 10 practical examples.LlamaIndexBlogPartnersCareersContactStatusEnterpriseLlamaCloudLlamaParseOpen SourcePython packagePython docsTypeScript packageTypeScript docsLlamaHubGitHubCommunityNewsletterDiscordTwitter/XLinkedInYouTubeStarter projectscreate-llamaSEC InsightsChat LlamaIndexLlamaBotRAG CLI 2024 LlamaIndexPrivacy NoticeTerms of Service<br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "for node in base_nodes:\n",
        "    display_source_node(node, source_length=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 813
        },
        "id": "FS_jbmYX69xh",
        "outputId": "66933445-f104-4614-b8d1-aaebc5f67a63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Merging 2 nodes into parent node.\n",
            "> Parent node id: 4d7dabcf-ee2e-41af-90a1-3f428d597844.\n",
            "> Parent node text: Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems  Ll...\n",
            "\n",
            "> Merging 1 nodes into parent node.\n",
            "> Parent node id: e81e093e-bf96-419a-9acd-aea6a0265a5d.\n",
            "> Parent node text: His name is Peter.\")] index = VectorStoreIndex.from_documents(docs) # Define a query rewrite agen...\n",
            "\n",
            "> Merging 1 nodes into parent node.\n",
            "> Parent node id: 80ab5c6d-b8f9-46fa-aac6-7f1deec68513.\n",
            "> Parent node text: LlamaIndex Newsletter 2024-07-02  LlamaIndex, Data Framework for LLM ApplicationsEnterpriseOpen s...\n",
            "\n",
            "> Merging 2 nodes into parent node.\n",
            "> Parent node id: df506443-a581-4826-9d00-63229ca12281.\n",
            "> Parent node text: Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems  Ll...\n",
            "\n",
            "> Merging 1 nodes into parent node.\n",
            "> Parent node id: 71a33e83-9fdc-4ac6-a397-c046a2e638d4.\n",
            "> Parent node text: LlamaIndex Newsletter 2024-07-02  LlamaIndex, Data Framework for LLM ApplicationsEnterpriseOpen s...\n",
            "\n",
            "Sources:\n",
            "Title: Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
            " URL: https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems\n",
            "Text preview: Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems  LlamaIndex, Data Framework for LLM ApplicationsEnterpriseOpen sourceCommunityCareersBlog Talk to usEnterp \n",
            "\n",
            "\n",
            "Title: Data Agents\n",
            " URL: https://www.llamaindex.ai/blog/data-agents-eed797d7972f\n",
            "Text preview: For instance, you may want to define a specific workflow over text-to-SQL with our NLStructStoreQueryEngine (constrained), or a router module to decide between semantic search or summarization (less c \n",
            "\n",
            "\n",
            "Title: Data Agents\n",
            " URL: https://www.llamaindex.ai/blog/data-agents-eed797d7972f\n",
            "Text preview: Data Agents  LlamaIndex, Data Framework for LLM ApplicationsEnterpriseOpen sourceCommunityCareersBlog Talk to usEnterpriseOpen sourceCommunityCareersBlog Talk to us 2024 LlamaIndexJerry Liu  Jul 12, 2 \n",
            "\n",
            "\n",
            "Title: LlamaIndex Newsletter 2024-07-02\n",
            " URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-02\n",
            "Text preview: LlamaIndex Newsletter 2024-07-02  LlamaIndex, Data Framework for LLM ApplicationsEnterpriseOpen sourceCommunityCareersBlog Talk to usEnterpriseOpen sourceCommunityCareersBlog Talk to us 2024 LlamaInde \n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**`Final Response:`** The key features of llama-agents include a Distributed Service Oriented Architecture where each agent can function as an independently running microservice, standardized API interfaces for communication between agents, the ability to define agentic and explicit orchestration flows, ease of deployment with the capability to launch, scale, and monitor each agent independently, and built-in observability tools for scalability and resource management."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "query_engine = RetrieverQueryEngine.from_args(retriever, response_mode=\"refine\")\n",
        "base_query_engine = RetrieverQueryEngine.from_args(base_retriever, response_mode=\"refine\")\n",
        "\n",
        "response = query_with_sources(query_engine, query_str)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        },
        "id": "btpC6SMs-Y0d",
        "outputId": "46d2dc2d-ebc3-4b7f-d8b2-6b1e05f5837e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sources:\n",
            "Title: Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
            " URL: https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems\n",
            "Text preview: Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems  LlamaIndex, Data Framework for LLM ApplicationsEnterpriseOpen sourceCommunityCareersBlog Talk to usEnterp \n",
            "\n",
            "\n",
            "Title: Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
            " URL: https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems\n",
            "Text preview: His name is Peter.\")] index = VectorStoreIndex.from_documents(docs) # Define a query rewrite agent HYDE_PROMPT_STR = ( \"Please rewrite the following query to include more detail:\\n{query_str}\\n\" ) HYD \n",
            "\n",
            "\n",
            "Title: Data Agents\n",
            " URL: https://www.llamaindex.ai/blog/data-agents-eed797d7972f\n",
            "Text preview: For instance, you may want to define a specific workflow over text-to-SQL with our NLStructStoreQueryEngine (constrained), or a router module to decide between semantic search or summarization (less c \n",
            "\n",
            "\n",
            "Title: Data Agents\n",
            " URL: https://www.llamaindex.ai/blog/data-agents-eed797d7972f\n",
            "Text preview: Data Agents  LlamaIndex, Data Framework for LLM ApplicationsEnterpriseOpen sourceCommunityCareersBlog Talk to usEnterpriseOpen sourceCommunityCareersBlog Talk to us 2024 LlamaIndexJerry Liu  Jul 12, 2 \n",
            "\n",
            "\n",
            "Title: Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
            " URL: https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems\n",
            "Text preview: Lets bring in our dependencies and set up our control plane again:import dotenv dotenv.load_dotenv() from llama_agents import ( AgentService, AgentOrchestrator, ControlPlaneServer, SimpleMessageQueue, \n",
            "\n",
            "\n",
            "Title: LlamaIndex Newsletter 2024-07-02\n",
            " URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-02\n",
            "Text preview: LlamaIndex Newsletter 2024-07-02  LlamaIndex, Data Framework for LLM ApplicationsEnterpriseOpen sourceCommunityCareersBlog Talk to usEnterpriseOpen sourceCommunityCareersBlog Talk to us 2024 LlamaInde \n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**`Final Response:`** The key features of llama-agents include a Distributed Service Oriented Architecture where each agent can function as an independently running microservice, communication via standardized API interfaces facilitated by a central control plane orchestrator, the ability to define agentic and explicit orchestration flows for interactions between agents, ease of deployment with the capability to launch, scale, and monitor each agent and control plane independently, and scalability and resource management through built-in observability tools to monitor system performance and quality."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "base_response = query_with_sources(base_query_engine, query_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSW74ZtnAUSs"
      },
      "source": [
        "## Re-ranker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "3Wyt4-rkKqFM"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.postprocessor import LLMRerank\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core import QueryBundle\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "\n",
        "def get_retrieved_nodes(query_str, vector_top_k=10, reranker_top_n=3, with_reranker=False):\n",
        "    \"\"\"\n",
        "    Retrieve and optionally rerank nodes based on a query string.\n",
        "\n",
        "    Args:\n",
        "        query_str (str): The query string to search for.\n",
        "        vector_top_k (int, optional): Number of top results to retrieve from the vector index. Defaults to 10.\n",
        "        reranker_top_n (int, optional): Number of top results to keep after reranking. Defaults to 3.\n",
        "        with_reranker (bool, optional): Whether to apply reranking. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        List[NodeWithScore]: A list of retrieved (and optionally reranked) nodes with their scores.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    query_bundle = QueryBundle(query_str)\n",
        "    # configure retriever\n",
        "    retriever = VectorIndexRetriever(\n",
        "        index=index,\n",
        "        similarity_top_k=vector_top_k,\n",
        "    )\n",
        "    retrieved_nodes = retriever.retrieve(query_bundle)\n",
        "\n",
        "    if with_reranker:\n",
        "        # configure reranker\n",
        "        reranker = LLMRerank(\n",
        "            choice_batch_size=5,\n",
        "            top_n=reranker_top_n,\n",
        "        )\n",
        "        retrieved_nodes = reranker.postprocess_nodes(\n",
        "            retrieved_nodes, query_bundle\n",
        "        )\n",
        "\n",
        "    return retrieved_nodes\n",
        "\n",
        "\n",
        "def pretty_print(df):\n",
        "    \"\"\"\n",
        "    Display a pandas DataFrame as a formatted HTML table.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): The DataFrame to display.\n",
        "\n",
        "    Returns:\n",
        "        IPython.core.display.HTML: An HTML representation of the DataFrame.\n",
        "    \"\"\"\n",
        "    return display(HTML(df.to_html().replace(\"\\\\n\", \"<br>\")))\n",
        "\n",
        "\n",
        "def visualize_retrieved_nodes(nodes) -> None:\n",
        "    \"\"\"\n",
        "    Visualize a list of retrieved nodes as a formatted table.\n",
        "\n",
        "    This function creates a DataFrame from the nodes' scores and text content,\n",
        "    then displays it using the pretty_print function.\n",
        "\n",
        "    Args:\n",
        "        nodes (List[NodeWithScore]): A list of retrieved nodes with their scores.\n",
        "\n",
        "    Returns:\n",
        "        None: This function displays the result but does not return a value.\n",
        "    \"\"\"\n",
        "    result_dicts = []\n",
        "    for node in nodes:\n",
        "        result_dict = {\"Score\": node.score, \"Text\": node.node.get_text()}\n",
        "        result_dicts.append(result_dict)\n",
        "\n",
        "    pretty_print(pd.DataFrame(result_dicts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6teO4PmQ1vr"
      },
      "source": [
        "### Without re-rank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "GwtEUqWHNaxj"
      },
      "outputs": [],
      "source": [
        "new_nodes = get_retrieved_nodes(\n",
        "    \"What are key features of llama-agents?\",\n",
        "    vector_top_k=5,\n",
        "    with_reranker=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cpQf_sacNbKL",
        "outputId": "2a4a06d4-c343-4086-e3aa-cc99ec8bafb3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Score</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.656160</td>\n",
              "      <td>his name is peter.\")] index = vectorstoreindex.from_documents(docs) # define a query rewrite agent hyde_prompt_str = ( \"please rewrite the following query to include more detail:<br>{query_str}<br>\" ) hyde_prompt_tmpl = prompttemplate(hyde_prompt_str) def run_hyde_fn(state): prompt_tmpl, llm, input_str = ( state[\"prompt_tmpl\"], state[\"llm\"], state[\"__task__\"].input, ) qp = querypipeline(chain=[prompt_tmpl, llm]) output = qp.run(query_str=input_str) state[\"__output__\"] = str(output) return state, true hyde_agent = fnagentworker( fn=run_hyde_fn, initial_state={\"prompt_tmpl\": hyde_prompt_tmpl, \"llm\": openai()} ).as_agent() # define a rag agent def run_rag_fn(state): retriever, llm, input_str = ( state[\"retriever\"], state[\"llm\"], state[\"__task__\"].input, ) query_engine = retrieverqueryengine.from_args(retriever, llm=llm) response = query_engine.query(input_str) state[\"__output__\"] = str(response) return state, true rag_agent = fnagentworker( fn=run_rag_fn, initial_state={\"retriever\": index.as_retriever(), \"llm\": openai()} ).as_agent() # set up the multi-agent system message_queue = simplemessagequeue() query_rewrite_service = agentservice( agent=hyde_agent, message_queue=message_queue, description=\"query rewriting service\", service_name=\"query_rewrite\", ) rag_service = agentservice( agent=rag_agent, message_queue=message_queue, description=\"rag service\", service_name=\"rag\", ) # create the pipeline pipeline = querypipeline(chain=[ servicecomponent.from_service_definition(query_rewrite_service), servicecomponent.from_service_definition(rag_service), ]) orchestrator = pipelineorchestrator(pipeline) control_plane = controlplaneserver( message_queue=message_queue, orchestrator=orchestrator, ) # set up the launcher launcher = locallauncher( [query_rewrite_service, rag_service], control_plane, message_queue, ) # run a query result = launcher.launch_single(\"tell me about rabbits\") print(result)public roadmapthis is an alpha release, meaning that wed love your feedback on features to better help you build multi-agent systems in production! weve created a public roadmap showing where we plan to go from here. were actively seeking public feedback on what works for you and what doesnt.dive in!llama-agents provides a powerful, flexible framework for building complex multi-agent ai systems. whether you're prototyping a new idea or scaling to production, llama-agents offers the tools you need to bring your ai vision to life. check out the repo to learn more, especially our library of examples.we're excited to see what the community builds with llama-agents. happy coding!related articlesbuilding a multi-agent concierge system2024-07-17introducing llama-agents: a powerful framework for building production multi-agent ai systems2024-06-26automate online tasks with multion and llamaindex2024-05-23how to build llm agents in typescript with llamaindex.ts2024-02-08llamaindexblogpartnerscareerscontactstatusenterprisellamacloudllamaparseopen sourcepython packagepython docstypescript packagetypescript docsllamahubgithubcommunitynewsletterdiscordtwitter/xlinkedinyoutubestarter projectscreate-llamasec insightschat llamaindexllamabotrag cli 2024 llamaindexprivacy noticeterms of service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.601985</td>\n",
              "      <td>introducing llama-agents: a powerful framework for building production multi-agent ai systems  llamaindex, data framework for llm applicationsenterpriseopen sourcecommunitycareersblog talk to usenterpriseopen sourcecommunitycareersblog talk to us 2024 llamaindexllamaindex  jun 26, 2024introducing llama-agents: a powerful framework for building production multi-agent ai systemsagentswe're excited to announce the alpha release of llama-agents, a new open-source framework designed to simplify the process of building, iterating, and deploying multi-agent ai systems and turn your agents into production microservices. whether you're working on complex question-answering systems, collaborative ai assistants, or distributed ai workflows, llama-agents provides the tools and structure you need to bring your ideas to life.key features of llama-agentsdistributed service oriented architecture: every agent in llamaindex can be its own independently running microservice, orchestrated by a fully customizable llm-powered control plane that routes and distributes tasks.communication via standardized api interfaces: interface between agents using a central control plane orchestrator. pass messages between agents using a message queue.define agentic and explicit orchestration flows: developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an agentic orchestrator that decides which agents are relevant to the task.ease of deployment: launch, scale and monitor each agent and your control plane independently.scalability and resource management: use our built-in observability tools to monitor the quality and performance of the system and each individual agent servicelet's dive into how you can start using llama-agents to build your own multi-agent systems.getting started with llama-agentsfirst, install the framework using pip:pip install llama-agents llama-index-agent-openaibasic system setuphere's a simple example of how to set up a basic multi-agent system using llama-agents. first well bring in our dependencies and set up our control plane, which contains our llm-powered orchestratorimport dotenv dotenv.load_dotenv() # our .env file defines openai_api_key from llama_agents import ( agentservice, controlplaneserver, simplemessagequeue, agentorchestrator, ) from llama_index.core.agent import functioncallingagentworker from llama_index.core.tools import functiontool from llama_index.llms.openai import openai import logging # turn on logging so we can see the system working logging.getlogger(\"llama_agents\").setlevel(logging.info) # set up the message queue and control plane message_queue = simplemessagequeue() control_plane = controlplaneserver( message_queue=message_queue, orchestrator=agentorchestrator(llm=openai()), )next we create our tools using llamaindexs existing abstractions, provide those tools to an agent, and turn that agent into an independent microservice:# create a tool def get_the_secret_fact() -&gt; str: \"\"\"returns the secret fact.\"\"\" return \"the secret fact is: a baby llama is called a 'cria'.\" tool = functiontool.from_defaults(fn=get_the_secret_fact) # define an agent worker = functioncallingagentworker.from_tools([tool], llm=openai()) agent = worker.as_agent() # create an agent service agent_service = agentservice( agent=agent, message_queue=message_queue, description=\"general purpose assistant\", service_name=\"assistant\", )finally we launch the service and the control plane. note that here were using a helper function to run a single query through the system and then exit; next well show how to deploy this to production.# set up the launcher for local testing from llama_agents import locallauncher launcher = locallauncher( [agent_service], control_plane, message_queue, ) # run a single query through the system result = launcher.launch_single(\"what's the secret fact?\") print(result)deploying your multi-agent systemonce you've tested your system locally, you can deploy it as a set of services for real production use. here's how you might set that up. this is similar to the previous example, but weve added a second agent service and were using a different launcher. lets bring in our dependencies and set up our control plane again:import dotenv dotenv.load_dotenv() from llama_agents import ( agentservice, agentorchestrator, controlplaneserver, simplemessagequeue, ) from llama_index.core.agent import functioncallingagentworker from llama_index.core.tools import functiontool from llama_index.llms.openai import openai import logging # change logging level to enable or disable more verbose logging logging.getlogger(\"llama_agents\").setlevel(logging.info) # create our multi-agent framework components message_queue = simplemessagequeue() control_plane = controlplaneserver( message_queue=message_queue, orchestrator=agentorchestrator(llm=openai()), )then as before we create a tool and an agent, though this time well add a second agent:# create a tool def get_the_secret_fact() -&gt; str: \"\"\"returns the secret fact.\"\"\" return \"the secret fact is: a baby llama is called a 'cria'.\" tool = functiontool.from_defaults(fn=get_the_secret_fact) # create our agents worker1 = functioncallingagentworker.from_tools([tool], llm=openai()) worker2 = functioncallingagentworker.from_tools([], llm=openai()) agent1 = worker1.as_agent() agent2 = worker2.as_agent()we turn those agents into services:agent_server_1 = agentservice( agent=agent1, message_queue=message_queue, description=\"useful for getting the secret fact.\", service_name=\"secret_fact_agent\", host=\"localhost\", port=8003 ) agent_server_2 = agentservice( agent=agent2, message_queue=message_queue, description=\"useful for getting random dumb facts.\", service_name=\"dumb_fact_agent\", host=\"localhost\", port=8004 )and finally we launch each service as an independent agent. here were doing them all from a single script, but each of these could be a totally separate service, launched and scaled independently:from llama_agents import serverlauncher, callablemessageconsumer # additional human consumer def handle_result(message) -&gt; none: print(f\"got result:\", message.data) # the final result is published to a \"human\" consumer # so we define one to handle it! human_consumer = callablemessageconsumer( handler=handle_result, message_type=\"human\" ) # define launcher launcher = serverlauncher( [agent_server_1, agent_server_2], control_plane, message_queue, additional_consumers=[human_consumer] ) launcher.launch_servers()real-time monitoringone of the coolest debugging features of our multi-agent system is our agent monitor, which is built right in. you launch it like this:llama-agents monitor --control-plane-url http://127.0.0.1:8000once launched, you get an intuitive, point-and-click terminal application. you can see both of the agents running, and at the bottom you can inject a task like the query what is the secret fact? youll get a job id which you can then click on to see your results:building a query rewriting rag systemlet's look at a more complex example: a query rewriting rag system. this system will rewrite user queries to improve retrieval, then use the rewritten query to perform rag over a document.this example demonstrates how to create a more sophisticated system that combines query rewriting with rag to improve question-answering capabilities. see this notebook for a fuller explanation of whats going on.import dotenv dotenv.load_dotenv() # our .env defines openai_api_key from llama_index.core import vectorstoreindex, document from llama_index.core.agent import fnagentworker from llama_index.core import prompttemplate from llama_index.core.query_pipeline import querypipeline from llama_index.core.query_engine import retrieverqueryengine from llama_agents import ( agentservice, controlplaneserver, simplemessagequeue, pipelineorchestrator, servicecomponent, ) from llama_agents.launchers import locallauncher from llama_index.llms.openai import openai import logging # change logging level to enable or disable more verbose logging logging.getlogger(\"llama_agents\").setlevel(logging.info) # load and index your document docs = [document(text=\"the rabbit is a small mammal with long ears and a fluffy tail. his name is peter.\")]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.588961</td>\n",
              "      <td>for instance, you may want to define a specific workflow over text-to-sql with our nlstructstorequeryengine (constrained), or a router module to decide between semantic search or summarization (less constrained), or use our subquestionqueryengine to decompose a question among sub-documents (even less constrained).by default, agent loops are unconstrained, and can theoretically reason over any set of tools that you give them. this means that you can get out-of-the-box advanced search/retrieval capabilities  for instance, in our openai cookbook we show that you can get joint text-to-sql capabilities by simply providing a sql query engine and vector store query engine as tools. but on the other hand, agents built in this fashion can be quite unreliable (see our blog post for more insights). if you are using agents for search/retrieval, be mindful of the 1) llm you pick, and the 2) set of tools you pick too.how are llamaindex data agents different than existing agent frameworks (langchain, hugging face, etc.)?most of these core concepts are not new. our overall design has taken inspiration from popular tools and frameworks for building agents. but in our data agents design, weve tried our best to answer the following key questions well:how do we effectively index/query and retrieve data beforehand?how do we effectively index/query and retrieve data on the fly?how do we design api interfaces for read/writes that are simultaneously rich (can take in structured inputs), but also easy for agents to understand?how do we properly get sources in citations?our goal with data agents is to create automated knowledge workers that can reason over and interact with data. our core toolkit provides the foundations for properly indexing, retrieving, and querying data  these can be easily integrated as tools. we provide some additional tool abstractions to handle the cases where you want to cache api outputs on the fly (see above). finally, we provide principled tool abstractions and design principles so that agents can interface with external services in a structured manner.can i use tools with langchain agents? you can easily use any of our tools with langchain agents as well.tools = tool_spec.to_tool_list() langchain_tools = [t.to_langchain_tool() for t in tools]see our tools usage guide for more details!conclusionin summary, today we launched two key items: data agent components (incl. agent reasoning loop and tool abstractions) and the llamahub tool repository.resourcesweve written a comprehensive section in the docs  take a look here: https://gpt-index.readthedocs.io/en/latest/core_modules/agent_modules/agents/root.htmltake a look at our llamahub tools section: https://llamahub.ai/notebook tutorials for llamahub tools: https://github.com/emptycrown/llama-hub/tree/main/llama_hub/tools/notebooksif you have questions, please hop on our discord: https://discord.gg/dgcwcsnxhurelated articlesbuilding a multi-agent concierge system2024-07-17introducing llama-agents: a powerful framework for building production multi-agent ai systems2024-06-26automate online tasks with multion and llamaindex2024-05-23supercharge your llamaindex rag pipeline with uptrain evaluations2024-03-19llamaindexblogpartnerscareerscontactstatusenterprisellamacloudllamaparseopen sourcepython packagepython docstypescript packagetypescript docsllamahubgithubcommunitynewsletterdiscordtwitter/xlinkedinyoutubestarter projectscreate-llamasec insightschat llamaindexllamabotrag cli 2024 llamaindexprivacy noticeterms of service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.585260</td>\n",
              "      <td>i hope that these reflections and techniques assist you in creating new tools for agents, and dont forget to share your tools on llamahub.related articlesbuilding a multi-agent concierge system2024-07-17introducing llama-agents: a powerful framework for building production multi-agent ai systems2024-06-26automate online tasks with multion and llamaindex2024-05-23llamaindex newsletter 2024-04-022024-04-02llamaindexblogpartnerscareerscontactstatusenterprisellamacloudllamaparseopen sourcepython packagepython docstypescript packagetypescript docsllamahubgithubcommunitynewsletterdiscordtwitter/xlinkedinyoutubestarter projectscreate-llamasec insightschat llamaindexllamabotrag cli 2024 llamaindexprivacy noticeterms of service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.569715</td>\n",
              "      <td>llamaindex newsletter 2024-07-02  llamaindex, data framework for llm applicationsenterpriseopen sourcecommunitycareersblog talk to usenterpriseopen sourcecommunitycareersblog talk to us 2024 llamaindexllamaindex  jul 2, 2024llamaindex newsletter 2024-07-02hello, llama enthusiasts! welcome to this weeks edition of the llamaindex newsletter! in this issue, were excited to bring you exciting updates about llama-agents, live demos, extensive guides, and in-depth tutorials to enhance your understanding of our tools.before moving into our newsletter, we have an exciting update on our enterprise offerings. we are thrilled to announce the waitlist release of llamacloud, our fully-managed ingestion service. sign up now if youre eager to collaborate and build llm applications with llamacloud. the highlights:launched llama-agents framework: our new alpha-release, llama-agents, enables multi-agent ai systems for production with a distributed architecture, http api communication, and agentic orchestration. its designed for easy deployment, scalability, and observability. blogpost, tweet.create-llama integrated with llamacloud: streamline your llm application data pipelines with create-llama, now integrated with llamacloud for faster setup and efficient system maintenance. tweet. feature releases and enhancements:we have launched llama-agents - new alpha-release framework that enables multi-agent ai systems to go into production. it features a distributed, service-oriented architecture, communication through standard http apis, agentic orchestration of flows, and is designed for easy deployment, scalability, and observability. blogpost, tweet.create-llama is now integrated with llamacloud to streamline the setup and management of data pipelines for llm applications, providing a fast and efficient way to deploy and maintain these systems. tweet.we have integrated with dspy for optimized rag by utilizing dspys optimization capabilities with llamaindexs data tools to enhance your query pipelines, optimize prompts, or repurpose dspy predictors. cookbook, tweet. demos:automating code reviews, project by composio with llamaindex automates code reviews using an ai agent in under 100 lines of code that monitors github prs, reviews them immediately upon creation, and posts feedback directly to your slack channel. codebase. guides:guide to building an agentic rag service with our comprehensive notebook that walks you through creating vector indexes, transforming them into query engines, turning each engine into a tool, providing these tools to agents, and launching the agents as services.guide to ai agents with llamaindex: andreis comprehensive workshop from gen ai philippines, showcasing llm applications through llamaindex. this beginner-friendly session covers topics from rag to multi-hop agents. video, notebook. tutorials:kingzzms tutorial on crafting a custom hybrid retriever using llamaindexs flexible abstractions. this tutorial teaches you how to integrate full text and dense search capabilities from elastic, and how to write your own reciprocal rank fusion function for optimal retrieval strategy.jeffs tutorial on which outlines the essential tools needed to construct a report generator using a react agent. learn how to integrate a rag tool over guideline documents, a web search tool, and a report generation tool that converts markdown text into pdfs.1littlecoders tutorial on llama-agents provides a detailed introduction to transforming multi-agent systems into microservices for production, including setup examples and a walkthrough of the architecture involving the control plane, message queue, and agent services using llamaindex abstractions.mervin praisons tutorial on the llama-agents framework provides a concise guide to setting up agent services, from notebook synchronization to server-client interactions, complete with over 10 practical examples.llamaindexblogpartnerscareerscontactstatusenterprisellamacloudllamaparseopen sourcepython packagepython docstypescript packagetypescript docsllamahubgithubcommunitynewsletterdiscordtwitter/xlinkedinyoutubestarter projectscreate-llamasec insightschat llamaindexllamabotrag cli 2024 llamaindexprivacy noticeterms of service</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "visualize_retrieved_nodes(new_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "id": "sF6DaO3GtCRo",
        "outputId": "7e977862-f574-4bc9-d88f-fe99b6bf7f99"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**`Final Response:`** The key features of llama-agents include a distributed service-oriented architecture where each agent can run independently as a microservice, communication through standardized API interfaces using a central control plane orchestrator, the ability to define agentic and explicit orchestration flows, ease of deployment with the capability to launch, scale, and monitor each agent and control plane independently, and built-in observability tools for monitoring the quality and performance of the system and individual agent services."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Sources:\n",
            "1. Title: Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
            " URL: https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems\n",
            "   Preview: his name is peter.\")] index = vectorstoreindex.from_documents(docs) # define a query rewrite agent hyde_prompt_str = ( \"please rewrite the following query to include more detail:\\n{query_str}\\n\" ) hyd...\n",
            "\n",
            "2. Title: Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
            " URL: https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems\n",
            "   Preview: introducing llama-agents: a powerful framework for building production multi-agent ai systems  llamaindex, data framework for llm applicationsenterpriseopen sourcecommunitycareersblog talk to usenterp...\n",
            "\n",
            "3. Title: Data Agents\n",
            " URL: https://www.llamaindex.ai/blog/data-agents-eed797d7972f\n",
            "   Preview: for instance, you may want to define a specific workflow over text-to-sql with our nlstructstorequeryengine (constrained), or a router module to decide between semantic search or summarization (less c...\n",
            "\n",
            "4. Title: Building Better Tools for LLM Agents\n",
            " URL: https://www.llamaindex.ai/blog/building-better-tools-for-llm-agents-f8c5a6714f11\n",
            "   Preview: i hope that these reflections and techniques assist you in creating new tools for agents, and dont forget to share your tools on llamahub.related articlesbuilding a multi-agent concierge system2024-07...\n",
            "\n",
            "5. Title: LlamaIndex Newsletter 2024-07-02\n",
            " URL: https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-02\n",
            "   Preview: llamaindex newsletter 2024-07-02  llamaindex, data framework for llm applicationsenterpriseopen sourcecommunitycareersblog talk to usenterpriseopen sourcecommunitycareersblog talk to us 2024 llamainde...\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def generate_response(query, retrieved_nodes):\n",
        "    # Initialize context and sources string\n",
        "    context = \"\"\n",
        "    sources = \"Sources:\\n\"\n",
        "\n",
        "    # Combine the text from all retrieved nodes and create sources information\n",
        "    for i, node in enumerate(retrieved_nodes, 1):\n",
        "        # Get the full text of the node\n",
        "        full_text = node.node.get_text()\n",
        "\n",
        "        title = node.node.metadata.get('title', 'Unknown Title')\n",
        "\n",
        "        # Get the URL from metadata\n",
        "        url = node.node.metadata.get('URL', 'URL not available')\n",
        "\n",
        "        # Add to context\n",
        "        context += f\"{full_text}\\n\\n\"\n",
        "\n",
        "        # Add to sources\n",
        "        sources += f\"{i}. Title: {title}\\n\"\n",
        "        sources += f\" URL: {url}\\n\"\n",
        "        sources += f\"   Preview: {full_text[:200]}...\\n\\n\"\n",
        "\n",
        "    # Create a prompt that includes the context and the query\n",
        "    prompt = f\"Based on the following information:\\n\\n{context}\\nPlease answer the question: {query}\"\n",
        "\n",
        "    # Generate a response using the query engine\n",
        "    response = query_engine.query(prompt)\n",
        "\n",
        "    # Combine the response with the sources information\n",
        "    full_response = f\"\\n\\n{sources}\"\n",
        "    display_response(response)\n",
        "    print(full_response)\n",
        "\n",
        "\n",
        "# Usage\n",
        "query = \"What are key features of llama-agents?\"\n",
        "new_nodes = get_retrieved_nodes(query, vector_top_k=5, with_reranker=False)\n",
        "response = generate_response(query, new_nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXABnNNWQ5ps"
      },
      "source": [
        "### With re-rank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "f6StRD11MNkj"
      },
      "outputs": [],
      "source": [
        "\n",
        "new_nodes = get_retrieved_nodes(\n",
        "    \"What are key features of llama-agents?\",\n",
        "    vector_top_k=10,\n",
        "    reranker_top_n=3,\n",
        "    with_reranker=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BHqkVgvSMkDO",
        "outputId": "aeea36a4-4b4c-4abd-fdc6-572815d6c0a0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Score</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10.0</td>\n",
              "      <td>introducing llama-agents: a powerful framework for building production multi-agent ai systems  llamaindex, data framework for llm applicationsenterpriseopen sourcecommunitycareersblog talk to usenterpriseopen sourcecommunitycareersblog talk to us 2024 llamaindexllamaindex  jun 26, 2024introducing llama-agents: a powerful framework for building production multi-agent ai systemsagentswe're excited to announce the alpha release of llama-agents, a new open-source framework designed to simplify the process of building, iterating, and deploying multi-agent ai systems and turn your agents into production microservices. whether you're working on complex question-answering systems, collaborative ai assistants, or distributed ai workflows, llama-agents provides the tools and structure you need to bring your ideas to life.key features of llama-agentsdistributed service oriented architecture: every agent in llamaindex can be its own independently running microservice, orchestrated by a fully customizable llm-powered control plane that routes and distributes tasks.communication via standardized api interfaces: interface between agents using a central control plane orchestrator. pass messages between agents using a message queue.define agentic and explicit orchestration flows: developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an agentic orchestrator that decides which agents are relevant to the task.ease of deployment: launch, scale and monitor each agent and your control plane independently.scalability and resource management: use our built-in observability tools to monitor the quality and performance of the system and each individual agent servicelet's dive into how you can start using llama-agents to build your own multi-agent systems.getting started with llama-agentsfirst, install the framework using pip:pip install llama-agents llama-index-agent-openaibasic system setuphere's a simple example of how to set up a basic multi-agent system using llama-agents. first well bring in our dependencies and set up our control plane, which contains our llm-powered orchestratorimport dotenv dotenv.load_dotenv() # our .env file defines openai_api_key from llama_agents import ( agentservice, controlplaneserver, simplemessagequeue, agentorchestrator, ) from llama_index.core.agent import functioncallingagentworker from llama_index.core.tools import functiontool from llama_index.llms.openai import openai import logging # turn on logging so we can see the system working logging.getlogger(\"llama_agents\").setlevel(logging.info) # set up the message queue and control plane message_queue = simplemessagequeue() control_plane = controlplaneserver( message_queue=message_queue, orchestrator=agentorchestrator(llm=openai()), )next we create our tools using llamaindexs existing abstractions, provide those tools to an agent, and turn that agent into an independent microservice:# create a tool def get_the_secret_fact() -&gt; str: \"\"\"returns the secret fact.\"\"\" return \"the secret fact is: a baby llama is called a 'cria'.\" tool = functiontool.from_defaults(fn=get_the_secret_fact) # define an agent worker = functioncallingagentworker.from_tools([tool], llm=openai()) agent = worker.as_agent() # create an agent service agent_service = agentservice( agent=agent, message_queue=message_queue, description=\"general purpose assistant\", service_name=\"assistant\", )finally we launch the service and the control plane. note that here were using a helper function to run a single query through the system and then exit; next well show how to deploy this to production.# set up the launcher for local testing from llama_agents import locallauncher launcher = locallauncher( [agent_service], control_plane, message_queue, ) # run a single query through the system result = launcher.launch_single(\"what's the secret fact?\") print(result)deploying your multi-agent systemonce you've tested your system locally, you can deploy it as a set of services for real production use. here's how you might set that up. this is similar to the previous example, but weve added a second agent service and were using a different launcher. lets bring in our dependencies and set up our control plane again:import dotenv dotenv.load_dotenv() from llama_agents import ( agentservice, agentorchestrator, controlplaneserver, simplemessagequeue, ) from llama_index.core.agent import functioncallingagentworker from llama_index.core.tools import functiontool from llama_index.llms.openai import openai import logging # change logging level to enable or disable more verbose logging logging.getlogger(\"llama_agents\").setlevel(logging.info) # create our multi-agent framework components message_queue = simplemessagequeue() control_plane = controlplaneserver( message_queue=message_queue, orchestrator=agentorchestrator(llm=openai()), )then as before we create a tool and an agent, though this time well add a second agent:# create a tool def get_the_secret_fact() -&gt; str: \"\"\"returns the secret fact.\"\"\" return \"the secret fact is: a baby llama is called a 'cria'.\" tool = functiontool.from_defaults(fn=get_the_secret_fact) # create our agents worker1 = functioncallingagentworker.from_tools([tool], llm=openai()) worker2 = functioncallingagentworker.from_tools([], llm=openai()) agent1 = worker1.as_agent() agent2 = worker2.as_agent()we turn those agents into services:agent_server_1 = agentservice( agent=agent1, message_queue=message_queue, description=\"useful for getting the secret fact.\", service_name=\"secret_fact_agent\", host=\"localhost\", port=8003 ) agent_server_2 = agentservice( agent=agent2, message_queue=message_queue, description=\"useful for getting random dumb facts.\", service_name=\"dumb_fact_agent\", host=\"localhost\", port=8004 )and finally we launch each service as an independent agent. here were doing them all from a single script, but each of these could be a totally separate service, launched and scaled independently:from llama_agents import serverlauncher, callablemessageconsumer # additional human consumer def handle_result(message) -&gt; none: print(f\"got result:\", message.data) # the final result is published to a \"human\" consumer # so we define one to handle it! human_consumer = callablemessageconsumer( handler=handle_result, message_type=\"human\" ) # define launcher launcher = serverlauncher( [agent_server_1, agent_server_2], control_plane, message_queue, additional_consumers=[human_consumer] ) launcher.launch_servers()real-time monitoringone of the coolest debugging features of our multi-agent system is our agent monitor, which is built right in. you launch it like this:llama-agents monitor --control-plane-url http://127.0.0.1:8000once launched, you get an intuitive, point-and-click terminal application. you can see both of the agents running, and at the bottom you can inject a task like the query what is the secret fact? youll get a job id which you can then click on to see your results:building a query rewriting rag systemlet's look at a more complex example: a query rewriting rag system. this system will rewrite user queries to improve retrieval, then use the rewritten query to perform rag over a document.this example demonstrates how to create a more sophisticated system that combines query rewriting with rag to improve question-answering capabilities. see this notebook for a fuller explanation of whats going on.import dotenv dotenv.load_dotenv() # our .env defines openai_api_key from llama_index.core import vectorstoreindex, document from llama_index.core.agent import fnagentworker from llama_index.core import prompttemplate from llama_index.core.query_pipeline import querypipeline from llama_index.core.query_engine import retrieverqueryengine from llama_agents import ( agentservice, controlplaneserver, simplemessagequeue, pipelineorchestrator, servicecomponent, ) from llama_agents.launchers import locallauncher from llama_index.llms.openai import openai import logging # change logging level to enable or disable more verbose logging logging.getlogger(\"llama_agents\").setlevel(logging.info) # load and index your document docs = [document(text=\"the rabbit is a small mammal with long ears and a fluffy tail. his name is peter.\")]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10.0</td>\n",
              "      <td>data agents  llamaindex, data framework for llm applicationsenterpriseopen sourcecommunitycareersblog talk to usenterpriseopen sourcecommunitycareersblog talk to us 2024 llamaindexjerry liu  jul 12, 2023data agentsllamaindexnlpaiagentsdatatoday were incredibly excited to announce the launch of a big new capability within llamaindex: data agents.data agents are llm-powered knowledge workers that can intelligently perform various tasks over your data, in both a read and write function. they are capable of the following:perform automated search and retrieval over different types of data  unstructured, semi-structured, and structured.calling any external service api in a structured fashion. they can either process the response immediately, or index/cache this data for future use.storing conversation history.using all of the above to fulfill both simple and complex data tasks.weve worked hard to provide abstractions, services, and guides on both the agents side and tools side in order to build data agents. todays launch consists of the following key components:general agent/tool abstractions: a set of abstractions to build agent loops, and to have those loops interact with tools according to a structured api definition.llamahub tool repository: a brand-new section within llamahub that consists of 15+ tools (e.g. google calendar, notion, sql, openapi) that can be connected. opening to community contributions!see below for full details. we show you how to build a gmail agent thats able to automatically create/send emails in &lt;10 lines of code!contextour core mission at llamaindex is to unlock the full capabilities of llms over your external sources of data. it provides a set of tools to both define state (how to parse/structure your data), and compute (how to query your data). up until now, our framework has primarily focused on search and retrieval use case. we have an incredible suite of tools and capabilities that not only allow you to create the basic rag stack around a vector database + top-k retrieval, but also offer much greater functionality beyond that.a lot of that technology used to lie in our query engines. our goal was to increase the capability of query engines to answer a wide range of different queries. in order to do this, we had to improve the reasoning capabilities of these query engines. as a result some of our existing query capabilities contain agent-like components: we have query engines capable of chain-of-thought reasoning, query decomposition, and routing. in the process, users had the option of choosing from a spectrum of query engines that had more constrained reasoning capabilities to less constrained capabilities.but there was a huge opportunity for llms to have an even richer set of interactions with data; they should be capable of general reasoning over any set of tools, whether from a database or an api. they should also be capable of both read and write capabilities  the ability to not only understand state but also modify it. as a result they should be able to do more than search and retrieval from a static knowledge source.some existing services, toolkits, and research papers have already demonstrated the possibilities of llm-powered agents that can interact with the external environment. using these existing approaches as inspiration, we saw an opportunity to build a principled series of abstractions enabling anyone to build knowledge workers over their data.core components of data agentsbuilding a data agent requires the following core components:a reasoning looptool abstractionsat a high-level, a data agent is provided with a set of apis, or tools, to interact with. these apis can return information about the world, or perform an action that modifies state. each tool exposes a request/response interface. the request is a set of structured parameters, and the response can be any format (at least conceptually, in most cases the response here is a text string of some form).given an input task, the data agent uses a reasoning loop to decide which tools to use, in which sequence, and the parameters to call each tool. the loop can conceptually be very simple (a one-step tool selection process), or complex (a multi-step selection process, where a multitude of tools are picked at each step).these components are described in more detail below.agent abstraction + reasoning loopwe have support for the following agents:openai function agent (built on top of the openai function api)a react agent (which works across any chat/text completion endpoint).you can use them as the following:from llama_index.agent import openaiagent, reactagent from llama_index.llms import openai # import and define tools ... # initialize llm llm = openai(model=\"gpt-3.5-turbo-0613\") # initialize openai agent agent = openaiagent.from_tools(tools, llm=llm, verbose=true) # initialize react agent agent = reactagent.from_tools(tools, llm=llm, verbose=true) # use agent response = agent.chat(\"what is (121 * 3) + 42?\")each agent takes in a set of tools. the details behind our tool abstractions are provided below. each agent also supports two main methods for taking in an input task  chat and query. note that these are the core methods used in our chatengine and queryengine respectively. in fact that our base agent class (baseagent) simply inherits from basechatengine and basequeryengine. chat allows the agent to utilize previously stored conversation history, whereas query is a stateless call - history/state is not preserved over time.the reasoning loop depends on the type of agent. the openai agent calls the openai function api in a while loop, since the tool decision logic is baked into the function api. given an input prompt and previous chat history (which includes previous function calls), the function api will decide whether to make another function call (pick a tool), or return an assistant message. if the api returns a function call, then we are responsible for executing the function and passing in a function message in the chat history. if the api returns an assistant message, then the loop is complete (we assume the task is solved).the react agent uses general text completion endpoints, so it can be used with any llm. a text completion endpoint has a simple input str  output str format, which means that the reasoning logic must be encoded in the prompt. the react agent uses an input prompt inspired by the react paper (and adapted into other versions), in order to decide which tool to pick. it looks something like this:... you have access to the following tools: {tool_desc} to answer the question, please use the following format. ``` thought: i need to use a tool to help me answer the question. action: tool name (one of {tool_names}) action input: the input to the tool, in a json format representing the kwargs (e.g. {{\"text\": \"hello world\", \"num_beams\": 5}}) ``` please use a valid json format for the action input. do not do this {{'text': 'hello world', 'num_beams': 5}}. if this format is used, you will receive a response in the following format: ``` observation: tool response ``` ...we implement react natively over chat prompts; the reasoning loop is implemented as an alternating series of assistant and user messages. the thought/action/action input section is represented as an assistant message, and the observation section is implemented as a user message.note: the react prompt expects not only the name of the tool to pick, but also the parameters to fill in the tool in a json format. this makes the output not dissimilar from the output of the openai function api  the main difference is that in the case of the function api, the tool-picking logic is baked into the api itself (through a finetuned model), whereas here it is elicited through explicit prompting.tool abstractionshaving proper tool abstractions is at the core of building data agents. defining a set of tools is similar to defining any api interface, with the exception that these tools are meant for agent rather than human use. we allow users to define both a single tool as well as a toolspec containing a series of functions under the hood.we describe the base tool abstraction, as well as how you can easily define tools over existing query engines, other tools.base tool abstractionthe base tool defines a very generic interface. the __call__ function can take in any series of arguments, and return a generic tooloutput container that can capture any response. a tool also has metadata containing its name, description, and function schema.@dataclass class toolmetadata: description: str name: optional[str] = none fn_schema: optional[type[basemodel]] = defaulttoolfnschema class basetool: @property @abstractmethod def metadata(self) -&amp;gt; toolmetadata: pass @abstractmethod def __call__(self, input: any) -&amp;gt; tooloutput: passfunction toola function tool allows users to easily convert any function into a tool. it takes in a user-defined function (that can take in any inputs/outputs), and wraps it into a tool interface.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.0</td>\n",
              "      <td>his name is peter.\")] index = vectorstoreindex.from_documents(docs) # define a query rewrite agent hyde_prompt_str = ( \"please rewrite the following query to include more detail:<br>{query_str}<br>\" ) hyde_prompt_tmpl = prompttemplate(hyde_prompt_str) def run_hyde_fn(state): prompt_tmpl, llm, input_str = ( state[\"prompt_tmpl\"], state[\"llm\"], state[\"__task__\"].input, ) qp = querypipeline(chain=[prompt_tmpl, llm]) output = qp.run(query_str=input_str) state[\"__output__\"] = str(output) return state, true hyde_agent = fnagentworker( fn=run_hyde_fn, initial_state={\"prompt_tmpl\": hyde_prompt_tmpl, \"llm\": openai()} ).as_agent() # define a rag agent def run_rag_fn(state): retriever, llm, input_str = ( state[\"retriever\"], state[\"llm\"], state[\"__task__\"].input, ) query_engine = retrieverqueryengine.from_args(retriever, llm=llm) response = query_engine.query(input_str) state[\"__output__\"] = str(response) return state, true rag_agent = fnagentworker( fn=run_rag_fn, initial_state={\"retriever\": index.as_retriever(), \"llm\": openai()} ).as_agent() # set up the multi-agent system message_queue = simplemessagequeue() query_rewrite_service = agentservice( agent=hyde_agent, message_queue=message_queue, description=\"query rewriting service\", service_name=\"query_rewrite\", ) rag_service = agentservice( agent=rag_agent, message_queue=message_queue, description=\"rag service\", service_name=\"rag\", ) # create the pipeline pipeline = querypipeline(chain=[ servicecomponent.from_service_definition(query_rewrite_service), servicecomponent.from_service_definition(rag_service), ]) orchestrator = pipelineorchestrator(pipeline) control_plane = controlplaneserver( message_queue=message_queue, orchestrator=orchestrator, ) # set up the launcher launcher = locallauncher( [query_rewrite_service, rag_service], control_plane, message_queue, ) # run a query result = launcher.launch_single(\"tell me about rabbits\") print(result)public roadmapthis is an alpha release, meaning that wed love your feedback on features to better help you build multi-agent systems in production! weve created a public roadmap showing where we plan to go from here. were actively seeking public feedback on what works for you and what doesnt.dive in!llama-agents provides a powerful, flexible framework for building complex multi-agent ai systems. whether you're prototyping a new idea or scaling to production, llama-agents offers the tools you need to bring your ai vision to life. check out the repo to learn more, especially our library of examples.we're excited to see what the community builds with llama-agents. happy coding!related articlesbuilding a multi-agent concierge system2024-07-17introducing llama-agents: a powerful framework for building production multi-agent ai systems2024-06-26automate online tasks with multion and llamaindex2024-05-23how to build llm agents in typescript with llamaindex.ts2024-02-08llamaindexblogpartnerscareerscontactstatusenterprisellamacloudllamaparseopen sourcepython packagepython docstypescript packagetypescript docsllamahubgithubcommunitynewsletterdiscordtwitter/xlinkedinyoutubestarter projectscreate-llamasec insightschat llamaindexllamabotrag cli 2024 llamaindexprivacy noticeterms of service</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "visualize_retrieved_nodes(new_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "OI-zuvE1rkgF",
        "outputId": "1edae6ef-6455-4f67-aa33-d1eff8aae442"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**`Final Response:`** The key features of llama-agents include a distributed service-oriented architecture where each agent can run independently as a microservice, communication via standardized API interfaces using a central control plane orchestrator, the ability to define agentic and explicit orchestration flows, ease of deployment allowing for independent launching, scaling, and monitoring of each agent and the control plane, as well as built-in observability tools for monitoring the quality and performance of the system and individual agent services."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Sources:\n",
            "1. Title: Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
            " URL: https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems\n",
            "   Preview: his name is peter.\")] index = vectorstoreindex.from_documents(docs) # define a query rewrite agent hyde_prompt_str = ( \"please rewrite the following query to include more detail:\\n{query_str}\\n\" ) hyd...\n",
            "\n",
            "2. Title: Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
            " URL: https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems\n",
            "   Preview: introducing llama-agents: a powerful framework for building production multi-agent ai systems  llamaindex, data framework for llm applicationsenterpriseopen sourcecommunitycareersblog talk to usenterp...\n",
            "\n",
            "3. Title: Data Agents\n",
            " URL: https://www.llamaindex.ai/blog/data-agents-eed797d7972f\n",
            "   Preview: for instance, you may want to define a specific workflow over text-to-sql with our nlstructstorequeryengine (constrained), or a router module to decide between semantic search or summarization (less c...\n",
            "\n",
            "\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "def generate_response(query, retrieved_nodes):\n",
        "    # Initialize context and sources string\n",
        "    context = \"\"\n",
        "    sources = \"Sources:\\n\"\n",
        "\n",
        "    # Combine the text from all retrieved nodes and create sources information\n",
        "    for i, node in enumerate(retrieved_nodes, 1):\n",
        "        # Get the full text of the node\n",
        "        full_text = node.node.get_text()\n",
        "\n",
        "        title = node.node.metadata.get('title', 'Unknown Title')\n",
        "\n",
        "        # Get the URL from metadata\n",
        "        url = node.node.metadata.get('URL', 'URL not available')\n",
        "\n",
        "        # Add to context\n",
        "        context += f\"{full_text}\\n\\n\"\n",
        "\n",
        "        # Add to sources\n",
        "        sources += f\"{i}. Title: {title}\\n\"\n",
        "        sources += f\" URL: {url}\\n\"\n",
        "        sources += f\"   Preview: {full_text[:200]}...\\n\\n\"\n",
        "\n",
        "    # Create a prompt that includes the context and the query\n",
        "    prompt = f\"Based on the following information:\\n\\n{context}\\nPlease answer the question: {query}\"\n",
        "\n",
        "    # Generate a response using the query engine\n",
        "    response = query_engine.query(prompt)\n",
        "\n",
        "    # Combine the response with the sources information\n",
        "    full_response = f\"\\n\\n{sources}\"\n",
        "    display_response(response)\n",
        "    print(full_response)\n",
        "\n",
        "\n",
        "# Usage\n",
        "query = \"What are key features of llama-agents?\"\n",
        "new_nodes = get_retrieved_nodes(query, vector_top_k=3, with_reranker=False)\n",
        "response = generate_response(query, new_nodes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh1eWPR3ZWXH"
      },
      "source": [
        "## HYDE\n",
        "\n",
        "HyDE is a technique where given a natural language query, a hypothetical document/answer is generated first. This hypothetical document is then used for embedding lookup rather than the raw query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlHo6N9VbMK1"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
        "from llama_index.core.query_engine import TransformQueryEngine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8zXcjRWZZnU"
      },
      "outputs": [],
      "source": [
        "query_str = \"What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN23K8Fza63X"
      },
      "source": [
        "### Without hyde"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "NIIztRaHZ43X",
        "outputId": "e5a63bf3-a86f-457c-cf0f-49077c43773a"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**`Final Response:`** The two main metrics used to evaluate the performance of the different rerankers in the RAG system are Hit Rate and Mean Reciprocal Rank (MRR)."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(query_str)\n",
        "display_response(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnaNmBXlbAhF"
      },
      "source": [
        "### With HYDE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FsADTlda6Ef"
      },
      "outputs": [],
      "source": [
        "hyde = HyDEQueryTransform(include_original=True)\n",
        "hyde_query_engine = TransformQueryEngine(query_engine, hyde)\n",
        "response = hyde_query_engine.query(query_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "L-YJu1u9bQmZ",
        "outputId": "9c5edaf3-ae46-4087-b2a5-f12fda5caea1"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**`Final Response:`** The Open AI and Jina AI-Base embeddings, especially when paired with the Cohere Rerank/bge-reranker-large reranker, set the gold standard for both hit rate and MRR. The influence of rerankers, particularly Cohere Rerank/bge-reranker-large, cannot be overstated. They play a key role in improving the MRR for many embeddings, showing their importance in making search results better. Choosing the right embedding for the initial search is essential; even the best reranker can't help much if the basic search results aren't good. To get the best out of retrievers, it's important to find the right mix of embeddings and rerankers."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display_response(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "Tppfq-x1dfmO",
        "outputId": "1f142371-14a2-4298-d7cf-8818548a6609"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The two main metrics used to evaluate the performance of the different rerankers in the RAG system are BLEU score and ROUGE score. BLEU score measures the similarity between the generated text and the reference text by comparing n-grams, while ROUGE score evaluates the quality of the generated text by measuring the overlap of n-grams and word sequences between the generated text and the reference text. These metrics help to assess the effectiveness of the rerankers in improving the relevance and coherence of the generated text in the RAG system. Additionally, other metrics such as perplexity, fluency, and diversity may also be used to provide a comprehensive evaluation of the rerankers\\' performance.\"\"\"'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query_bundle = hyde(query_str)\n",
        "hyde_doc = query_bundle.embedding_strs[0]\n",
        "hyde_doc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV1o0oyjeB2o"
      },
      "source": [
        "The query_bundle contains the original query and custom_embedding_strs which is the hypothetical document generated by the LLM in response to the query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdXse0DSequ6"
      },
      "source": [
        "## Multi-step query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAyX4Z3ue5Cg"
      },
      "outputs": [],
      "source": [
        "# LLM (gpt-3.5)\n",
        "gpt35 = OpenAI(temperature=0, model=\"gpt-3.5-turbo\",api_key=open_ai_key )\n",
        "\n",
        "# LLM (gpt-4o-mini)\n",
        "gpt4 = OpenAI(temperature=0, model=\"gpt-4o-mini\",api_key=open_ai_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmpktEI0evk9"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.indices.query.query_transform.base import (\n",
        "    StepDecomposeQueryTransform,\n",
        ")\n",
        "\n",
        "# gpt-4\n",
        "step_decompose_transform = StepDecomposeQueryTransform(llm=gpt4, verbose=True)\n",
        "\n",
        "# gpt-3\n",
        "step_decompose_transform_gpt3 = StepDecomposeQueryTransform(\n",
        "    llm=gpt35, verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxcYhuVoe64N",
        "outputId": "aef9a919-cf1e-4df9-d747-31c216d89b79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;3;33m> Current query: What are the two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?\n",
            "\u001b[0m\u001b[1;3;38;5;200m> New query: What specific metrics are used to evaluate RAG system performance in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?\n",
            "\u001b[0m\u001b[1;3;33m> Current query: What are the two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?\n",
            "\u001b[0m\u001b[1;3;38;5;200m> New query: What are the specific metrics used to evaluate RAG system performance in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?\n",
            "\u001b[0m\u001b[1;3;33m> Current query: What are the two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?\n",
            "\u001b[0m\u001b[1;3;38;5;200m> New query: What is the significance of Answer Relevancy in evaluating RAG system performance in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "index_summary = \"Used to answer questions about llamaindex\" #A string describing the purpose of the index\n",
        "\n",
        "from llama_index.core.query_engine import MultiStepQueryEngine\n",
        "\n",
        "query_engine = index.as_query_engine(llm=gpt35)\n",
        "query_engine = MultiStepQueryEngine(\n",
        "    query_engine=query_engine,\n",
        "    query_transform=step_decompose_transform_gpt3,\n",
        "    index_summary=index_summary,\n",
        ")\n",
        "response_gpt = query_engine.query(\n",
        "    \"What are the two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTqwXCkVgvez",
        "outputId": "7aba048f-5547-4892-93ad-f077338fa1cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer Relevancy and Context Relevancy are the two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook.\n"
          ]
        }
      ],
      "source": [
        "print(str(response_gpt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qY69pq9bhfv-",
        "outputId": "83cb8e1d-7a96-47b9-c85b-ddd41546ca8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(\"What specific metrics are used to evaluate RAG system performance in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?\", \"Answer: Answer Relevancy, Context Relevancy, Faithfulness, Retrieval Evaluation, and Batch Evaluations with Batch Eval Runner are specific metrics used to evaluate RAG system performance in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook.\"), (\"What are the specific metrics used to evaluate RAG system performance in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?\", \"Answer: Answer Relevancy, Context Relevancy, Faithfulness, Retrieval Evaluation, and Batch Evaluations with Batch Eval Runner are the specific metrics used to evaluate RAG system performance in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook.\"), (\"What is the significance of Answer Relevancy in evaluating RAG system performance in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?\", 'Answer Relevancy is crucial in evaluating RAG system performance as it helps assess how accurately the generated responses align with the questions posed by users. This aspect plays a key role in determining the effectiveness and quality of the RAG system in providing relevant and meaningful answers to user queries.')]\n"
          ]
        }
      ],
      "source": [
        "sub_qa = response_gpt.metadata[\"sub_qa\"]\n",
        "tuples = [(t[0], t[1].response) for t in sub_qa]\n",
        "print(tuples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iYeofUlS6_D"
      },
      "source": [
        "### Index Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCdR2_wmNol6"
      },
      "source": [
        "## Router Query Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHDiS6_AcID4"
      },
      "outputs": [],
      "source": [
        "vector_index = load_index_from_storage(storage_context, index_id=\"vector_index\")\n",
        "\n",
        "keyword_table_index = load_index_from_storage(storage_context, index_id=\"keyword_table_index\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axuO04J9SdZ_"
      },
      "outputs": [],
      "source": [
        "#selects one out of several candidate query engines to execute a query.\n",
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
        "\n",
        "vector_tool = QueryEngineTool(\n",
        "    vector_index.as_query_engine(response_mode=\"tree_summarize\"),\n",
        "    metadata=ToolMetadata(\n",
        "        name=\"vector_search\",\n",
        "        description=\"Useful for searching for specific facts.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "keyword_tool = QueryEngineTool(\n",
        "    query_engine=keyword_table_index.as_query_engine(),\n",
        "    metadata=ToolMetadata(\n",
        "        name=\"keyword_search\",\n",
        "        description=\"Useful for finding documents containing specific keywords or phrases.\"\n",
        "    )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7YhReZgNuoj"
      },
      "source": [
        "### Single Selector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "S7Dv6ENHNuI8",
        "outputId": "267254f9-5557-4214-b840-41d9e7171cca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sources:\n",
            "Title: Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
            " URL: https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems\n",
            "Text preview: his name is peter.\")] index = vectorstoreindex.from_documents(docs) # define a query rewrite agent hyde_prompt_str = ( \"please rewrite the following query to include more detail:\\n{query_str}\\n\" ) hyd \n",
            "\n",
            "\n",
            "Title: Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
            " URL: https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems\n",
            "Text preview: introducing llama-agents: a powerful framework for building production multi-agent ai systems  llamaindex, data framework for llm applicationsenterpriseopen sourcecommunitycareersblog talk to usenterp \n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**`Final Response:`** The key features of llama-agents include a distributed service-oriented architecture where each agent can function as an independent microservice, communication via standardized API interfaces facilitated by a central control plane orchestrator, the ability to define agentic and explicit orchestration flows, ease of deployment allowing for launching, scaling, and monitoring each agent independently, and built-in observability tools for monitoring the quality and performance of the system and individual agent services."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from llama_index.core.query_engine import RouterQueryEngine\n",
        "\n",
        "query_engine = RouterQueryEngine.from_defaults(\n",
        "    [vector_tool, keyword_tool],\n",
        "    select_multi=False\n",
        ")\n",
        "\n",
        "query_with_sources(query_engine, \"What are key features of llama-agents?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3kNeO6fdzOq"
      },
      "source": [
        "### Multi Selector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "bck-BvvIdWbC",
        "outputId": "f4715a72-0973-4a2c-dfe1-0a59d08d68db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sources:\n",
            "Title: Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
            " URL: https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems\n",
            "Text preview: his name is peter.\")] index = vectorstoreindex.from_documents(docs) # define a query rewrite agent hyde_prompt_str = ( \"please rewrite the following query to include more detail:\\n{query_str}\\n\" ) hyd \n",
            "\n",
            "\n",
            "Title: Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
            " URL: https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems\n",
            "Text preview: introducing llama-agents: a powerful framework for building production multi-agent ai systems  llamaindex, data framework for llm applicationsenterpriseopen sourcecommunitycareersblog talk to usenterp \n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**`Final Response:`** The key features of llama-agents include a distributed service-oriented architecture where each agent can function as an independent microservice, communication via standardized API interfaces facilitated by a central control plane orchestrator, the ability to define agentic and explicit orchestration flows, ease of deployment allowing for launching, scaling, and monitoring each agent and the control plane independently, as well as built-in observability tools for monitoring the quality and performance of the system and individual agent services."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from llama_index.core.query_engine import RouterQueryEngine\n",
        "\n",
        "query_engine = RouterQueryEngine.from_defaults(\n",
        "    [vector_tool, keyword_tool],\n",
        "    select_multi=True #allows the query engine to gather information from different sources (in this case, both the vector_tool and keyword_tool) and combine their outputs to form a more comprehensive answer.\n",
        ")\n",
        "\n",
        "query_with_sources(query_engine, \"What are key features of llama-agents?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNOc5g1Q3h6W"
      },
      "source": [
        "## Backoff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjIlFMch3kT3",
        "outputId": "af0c3fa4-a3aa-4849-d758-d921f6a82067"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello Phanh! I'm an AI language model created by OpenAI. How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "import logging.config\n",
        "import random\n",
        "import time\n",
        "from openai import OpenAI, RateLimitError\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "def retry_with_exponential_backoff(\n",
        "    func,\n",
        "    initial_delay: float = 1,\n",
        "    exponential_base: float = 2,\n",
        "    jitter: bool = True,\n",
        "    max_retries: int = 15,\n",
        "    errors: tuple = (RateLimitError,),\n",
        "):\n",
        "    \"\"\"Retry a function with exponential backoff.\"\"\"\n",
        "\n",
        "    def wrapper(*args, **kwargs):\n",
        "        # Initialize variables\n",
        "        num_retries = 0\n",
        "        delay = initial_delay\n",
        "\n",
        "        # Loop until a successful response or max_retries is hit or an exception is raised\n",
        "        while True:\n",
        "            try:\n",
        "                return func(*args, **kwargs)\n",
        "\n",
        "            # Retry on specific errors\n",
        "            except errors as e:\n",
        "                # Increment retries\n",
        "                num_retries += 1\n",
        "\n",
        "                # Check if max retries has been reached\n",
        "                if num_retries > max_retries:\n",
        "                    raise Exception(\n",
        "                        f\"Maximum number of retries ({max_retries}) exceeded.\"\n",
        "                    )\n",
        "\n",
        "                # Increment the delay\n",
        "                delay *= exponential_base * (1 + jitter * random.random())\n",
        "\n",
        "                logging.info(f\"Rate limit hit. Retrying in {delay:.2f} seconds...\")\n",
        "                # Sleep for the delay\n",
        "                time.sleep(delay)\n",
        "\n",
        "            # Raise exceptions for any errors not specified\n",
        "            except Exception as e:\n",
        "                raise e\n",
        "\n",
        "    return wrapper\n",
        "\n",
        "@retry_with_exponential_backoff\n",
        "def completions_with_backoff(**kwargs):\n",
        "    return openAI_client.chat.completions.create(**kwargs)\n",
        "\n",
        "openAI_client = OpenAI()\n",
        "\n",
        "response = completions_with_backoff(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"user\", \"content\": \"I am Phanh, who are you?\"}],\n",
        "            n=1,\n",
        "            temperature=0\n",
        "        )\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNCyphImjHff"
      },
      "source": [
        "#### Customize query engine using Backoff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gx4oCY-s97ay"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.query_engine import CustomQueryEngine\n",
        "from llama_index.core.retrievers import BaseRetriever\n",
        "from llama_index.core import get_response_synthesizer\n",
        "from llama_index.core.response_synthesizers import BaseSynthesizer\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "qa_prompt = PromptTemplate(\n",
        "    \"Context information is below.\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"{context_str}\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"Given the context information and not prior knowledge, \"\n",
        "    \"answer the query.\\n\"\n",
        "    \"Query: {query_str}\\n\"\n",
        "    \"Answer: \"\n",
        ")\n",
        "\n",
        "\n",
        "class RAGStringQueryEngine(CustomQueryEngine):\n",
        "    \"\"\"RAG String Query Engine.\"\"\"\n",
        "\n",
        "    retriever: BaseRetriever\n",
        "    response_synthesizer: BaseSynthesizer\n",
        "    llm: OpenAI\n",
        "    qa_prompt: PromptTemplate\n",
        "\n",
        "    @retry_with_exponential_backoff\n",
        "    def custom_query(self, query_str: str):\n",
        "        print(\"===custom query in progress===\")\n",
        "        nodes = self.retriever.retrieve(query_str)\n",
        "\n",
        "        context_str = \"\\n\\n\".join([n.node.get_content() for n in nodes])\n",
        "        response = self.llm.complete(\n",
        "            qa_prompt.format(context_str=context_str, query_str=query_str)\n",
        "        )\n",
        "\n",
        "        return str(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jelccFrc-Wlz"
      },
      "outputs": [],
      "source": [
        "synthesizer = get_response_synthesizer(response_mode=\"refine\")\n",
        "\n",
        "query_engine = RAGStringQueryEngine(\n",
        "    retriever=vector_index.as_retriever(),\n",
        "    response_synthesizer=synthesizer,\n",
        "    llm = OpenAI(model=\"gpt-3.5-turbo\"),\n",
        "    qa_prompt=qa_prompt,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "mKQyULtdlAC4",
        "outputId": "d90986de-6aeb-4793-e845-7e44d9f5d581"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===custom query in progress===\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**`Final Response:`** Some key features of llama-agents include:\n",
              "1. Distributed service oriented architecture: Each agent can be its own independently running microservice, orchestrated by a customizable llm-powered control plane.\n",
              "2. Communication via standardized API interfaces: Agents can interface with each other using a central control plane orchestrator and pass messages through a message queue.\n",
              "3. Define agentic and explicit orchestration flows: Developers have the flexibility to define the sequence of interactions between agents or leave it up to an agentic orchestrator.\n",
              "4. Ease of deployment: Agents and the control plane can be launched, scaled, and monitored independently.\n",
              "5. Scalability and resource management: Built-in observability tools allow monitoring of the system's quality and performance, as well as each individual agent service."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = query_engine.query(\"What are key features of llama-agents?\")\n",
        "display_response(response)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
